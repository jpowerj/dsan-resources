[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources for DSAN Students",
    "section": "",
    "text": "This page contains some general resources for DSAN students, which I hope can be useful across multiple different DSAN courses and/or in your future careers! ü§©\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Large Files in Projects\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nSo You‚Äôve Decided to Learn Python‚Ä¶\n\n\n(Opinionated) Foundations and Resources\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the World Bank‚Äôs Poverty Indicators\n\n\n\n\n\n\nDatasets\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nJeff‚Äôs Big Bucket of Datasets for Visualization\n\n\n\n\n\n\nDatasets\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Synthetic Microdata from Aggregated Count Data\n\n\n\n\n\n\nGeneral\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LIWC for Document-Level Sentiment Analysis\n\n\n\n\n\n\nText Analysis\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nEstablishing Global Settings for R in VSCode\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nKangheng Liu, Jeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nMounting Your GU Domains Space as a Local Drive\n\n\n\n\n\n\nGU Domains\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\nIncluding Images in .qmd Files\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i>Home"
    ]
  },
  {
    "objectID": "posts/learning-python/index.html#foundations",
    "href": "posts/learning-python/index.html#foundations",
    "title": "So You‚Äôve Decided to Learn Python‚Ä¶",
    "section": "Foundations",
    "text": "Foundations\nMy introductory spiel is just to beg you to consider the following three general points about learning computer science, before you dive head-first into your journey!\n\nThe Neuroscience of Learning: Focus on the Why, not the What\n\nFancy marketing agencies may have convinced you that the key to learning how to code is to have some sort of incredible (and expensive) resource like an app, textbook, or online class. However‚Ä¶\nCutting-edge neuroscientific studies of how we learn have shown that what really matters is the motivational aspect of your journey‚Äîmeaning, try your best to focus your energy on why you‚Äôre learning to code, rather than trying to optimize how (obsessing over the ‚Äúbest‚Äù resource to use) or what particular topics you‚Äôre studying.\n\nThe ultimate way to implement this point is to just have an idea of something you think would be fun to create using Python‚Äîdon‚Äôt worry about how difficult it may be, just have it in your head so that as you learn you can take mental notes like ‚ÄúOh! So that‚Äôs how [thing] works!‚Äù. When I was learning how to code, these were things like:\n\n‚ÄúWow, it‚Äôd be so cool if I could understand how exactly Instagram, TikTok, or WhatsApp work, by coding my own simplified version in Python‚Äù\n‚ÄúThis math explainer video is awesome, and it looks like they used Python to make it!?! How does that work? I should try to make my own video like this, as my motivation for learning Python‚Äù\n\n\n\nJeff‚Äôs Opinion Corner: Learn How to Code, not How to Code Python\nIt‚Äôs difficult to find truly reliable and transparent statistics on the rise and fall of various programming languages, but take a look at the following plot I put together for my Data Structures, Objects, and Algorithms course (DSAN 5500) at Georgetown:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nlang_df = pd.read_csv(\"assets/gh_issues.csv\")\n# The data for 2022 is essentially useless\nlang_df = lang_df[lang_df['year'] &lt;= 2021].copy()\nlang_df['time'] = lang_df['year'].astype(str) + \"_\" + lang_df['quarter'].astype(str)\nlang_df['prop'] = lang_df['count'] / lang_df.groupby('time')['count'].transform('sum')\nlang_df.head()\n#sns.lineplot(data=lang_df, x='year', y='count', color='name')\n# Keep only most popular languages\nkeep_langs = ['Python','JavaScript','C','C++','C#','Java','Ruby']\npop_df = lang_df[lang_df['name'].isin(keep_langs)].copy()\nfig = px.line(pop_df,\n  x='time', y='prop', color='name',\n  template='simple_white', title='Programming Language Popularity Since 2012',\n  labels = {\n    'time': 'Year',\n    'prop': 'Proportion of GitHub Issues'\n  }\n)\nfig.update_layout(\n  xaxis = dict(\n    tickmode = 'array',\n    tickvals = [f\"{year}_1\" for year in range(2012,2022)],\n    ticktext = [f\"{year}\" for year in range(2012,2022)]\n  )\n)\nfig.show()\n\n\n                                                \n\n\nNote how, for example:\n\nThe most popular programming language at the beginning of 2012 was Ruby, and then\nThe most popular programming language from mid-2012 until early 2020 was JavaScript, meaning that\nPython has only been the most popular programming language since early 2020\n\nAnd, if this plot could be extended back before 2012, into the 2000s and late 1990s, I think you‚Äôd see that Java1 was the most popular, and C++ before that, and C before that‚Ä¶\nWhich is all to say, please don‚Äôt put all your eggs in the Python basket! Instead of focusing on syntax that might be specific to Python, try your best to develop an understanding of the underlying logic and ways of thinking that apply across all programming languages, not just Python!\nSince I studied music before math or computer science, I think of it like:\n\n\n\n\n\n\n\n\nProgramming\nMusic\nCoolness Level\n\n\n\n\nLearning how to code in Python\nLearning how to play the French Horn üìØ\nMedium üôÇ‚Äç‚ÜïÔ∏è\n\n\nLearning how to code\nLearning how to play music üé∂ü•Åüéµüé§\nHIGH! ü•≥\n\n\n\nI like this metaphor because it highlights how, although you need to learn how to play some musical instrument in order to play music, you don‚Äôt have to restrict yourself to only making music with that instrument!\n\n\nProgramming Languages are Tools for Building Things, not Cults of Gatekeeping\nImagine you‚Äôre walking down the street when suddenly you encounter a bunch of horrifically pretentious people having a big argument:\n\nPerson A: No! You‚Äôre stupid! The hammer is clearly the best tool!\nPerson B: Smh, Person A. Someday you‚Äôll pull your head out of your ass and realize that the wrench is far superior to the hammer.\nPerson C: Why do I spend all my time with you two dummies‚Ä¶ Anyone with a brain can tell that the screwdriver absolutely shits on both the hammer and the wrench, in every way.\n\nThis would be pretty weird, right? And yet, that‚Äôs exactly what every argument about which programming languages were ‚Äúbetter‚Äù sounded like to me, throughout the entire six years of my BS and MS in Computer Science (just being honest!).\nHopefully, given this metaphor, my rant in the first half of this video (recorded for my department‚Äôs IG page) will click with you!"
  },
  {
    "objectID": "posts/learning-python/index.html#resources",
    "href": "posts/learning-python/index.html#resources",
    "title": "So You‚Äôve Decided to Learn Python‚Ä¶",
    "section": "Resources",
    "text": "Resources\nNow that you‚Äôve internalized those three lessons, you‚Äôre ready to explore the beautiful, verdant garden of resources that are available out there to help you learn Python‚Äîmost are totally free, and for the rest you can try pirating!\n\nInteractive Web Things\n\nfreeCodeCamp: Given everything I‚Äôve said above, hopefully it makes sense that I think this is totally all you need to get started on your Python journey! Rather than spending lots of money on a fancier thing (like the next few resources), try this and see how well it clicks with you!\nCS & Programming at Brilliant.org: I really really hate recommending things that cost money2, but I have to include this because\n\nI think it‚Äôs really good, it has a Duolingo-style interface that does the ultra-helpful thing of showing the ‚Äúpathway‚Äù of previously-learned, current, and next-up skills. But also\nIf it gets popular enough hopefully someone will make a free open-source knockoff using, e.g., Zoonk üòú\n\nDatacamp: I highlight this one because, although it‚Äôs another non-free resource, it can be very helpful if you know in advance that you want to focus in particular on using Python for data science (like, for example, if you find yourself as a new student in some sort of Data Science and Analytics program at Georgetown).\nTo this end, they offer following four-course sequence of Python-based courses:\n\nIntroduction to Python for Data Science\nPython Data Science Toolbox (Part 1)\nPython Data Science Toolbox (Part 2)\nObject-Oriented Programming in Python\n\n\n\n\nOld-Timey Paper Things\nFor me, since I care more about algorithms (the general study of how to get computers to do things for us) than I do about Python syntax, the holy grail of computer science textbooks is just this really simple, fun book with lots of pictures and metaphors and just beautiful keep-it-simple aesthetics:\n\nAditya Y. Bhargava, Grokking Algorithms\n\nThen, the following two textbooks were helpful when I was preparing the DSAN 5500 (Data Structures, Objects, and Algorithms in Python) course mentioned above, though target audience for these tends to be software engineers, who have slightly different needs from us data scientists!\n\nGoodrich, Michael T., Roberto Tamassia, and Michael H. Goldwasser. 2013. Data Structures and Algorithms in Python. [PDF] [EPUB]\nLee, Kent D., and Steve Hubbard. 2015. Data Structures and Algorithms with Python. [PDF] [EPUB]\n\nFinally, for much of DSAN 5500 we focused on a ‚Äústandard‚Äù collection of algorithms that all computer scientists (including data scientists!) should know; the most famous book collecting all of these algorithms into one place is known as ‚ÄúCLRS‚Äù, which is an abbreviation for the family names of the four authors (Cormen, Leiserson, Rivest, and Stein). The authors just released a Fourth Edition of the book in 2022, but the Third Edition is much easier to obtain, and honestly any edition should be fine for introductory learning!\n\nCormen, Thomas H., Charles E. Leiserson, Ronald R. Rivest, and Clifford Stein. 2022. Introduction to Algorithms, Fourth Edition. [PDF] [EPUB]"
  },
  {
    "objectID": "posts/learning-python/index.html#footnotes",
    "href": "posts/learning-python/index.html#footnotes",
    "title": "So You‚Äôve Decided to Learn Python‚Ä¶",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt‚Äôs a good thing to know now rather than later: Java has almost‚Ä¶ no relationship whatsoever to JavaScript, despite the names üòµ‚Äçüí´‚Ü©Ô∏é\nLike, to the extent that I made a whole website to try and dissuade people from paying for STEM things.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/rprofile-globals/index.html",
    "href": "posts/rprofile-globals/index.html",
    "title": "Establishing Global Settings for R in VSCode",
    "section": "",
    "text": "Awesome student Kangheng Liu figured out the following, which will be helpful for students who use the dark (default!) theme for VSCode but find that their R plots show up as black labels on a black background, so that they are not readable:\nThe IRkernel actually calls the R default startup process. The .Rprofile works whether inside or outside of jupyter.\nThe problem was R‚Äôs logic in sourcing the .Rprofile file. It won‚Äôt load libraries/packages in starting R kernel process. To pass background color as white, we need to write a hook for that. Search for hook in the link for explanations.\nThe coding is as below. in .Rprofile at home folder (or project folder, will take presendence if any):\nsetHook(packageEvent(\"grDevices\", \"onLoad\"),\n        function(...) grDevices::pdf.options(bg=\"white\"))\nThis will pass bg=\"white\" to grDevices, which is the R graphics device package.\nThank you Kangheng, on behalf of the DSAN students, for discovering this!"
  },
  {
    "objectID": "posts/synthetic-data/index.html",
    "href": "posts/synthetic-data/index.html",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province\n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province\n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "href": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province\n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province\n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-count-data",
    "href": "posts/synthetic-data/index.html#macro-level-count-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Macro-Level Count Data",
    "text": "Macro-Level Count Data\nSo, say you have a dataset that looks like the following, for example, obtained from the OECD‚Äôs Data Explorer portal1:\n\nlibrary(tidyverse)\nmacro_df &lt;- read_csv(\"assets/wild_life_cleaned.csv\")\nmacro_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIUCN\nIUCN Category\nSPEC\nSpecies\nCOU\nCountry\nValue\n\n\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUS\nAustralia\n377\n\n\nENDANGERED\nNumber of endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n41\n\n\nCRITICAL\nNumber of critically endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n9\n\n\nVULNERABLE\nNumber of vulnerable species\nMAMMAL\nMammals\nAUS\nAustralia\n57\n\n\nTHREATENED\nTotal number of threatened species\nMAMMAL\nMammals\nAUS\nAustralia\n107\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUT\nAustria\n104\n\n\n\n\n\n\nOnce we transform it from long to wide format, we‚Äôll be able to see how this dataset provides data on (country-species) pairs (e.g., Mammals in Australia), not on individual types of animals (e.g., Kangaroos in Australia):\n\nwide_df &lt;- macro_df |&gt;\n  select(-c(`IUCN Category`, COU, SPEC)) |&gt;\n  pivot_wider(\n    names_from = IUCN,\n    values_from = Value\n  ) |&gt;\n  select(-c(THREAT_PERCENT))\nwide_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nCountry\nTOT_KNOWN\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\n\n\n\n\nMammals\nAustralia\n377\n41\n9\n57\n107\n\n\nMammals\nAustria\n104\n6\n4\n17\n27\n\n\nMammals\nBelgium\n84\n2\n4\n12\n18\n\n\nMammals\nCanada\n222\n11\n11\n33\n55\n\n\nMammals\nCzech Republic\n92\n1\n10\n4\n15\n\n\nMammals\nDenmark\n71\n2\n1\n6\n9\n\n\n\n\n\n\nAlthough some methods like Decision Trees can handle both classification and regression tasks, there are other methods like ARM where using this type of count data can be difficult, since most of the methods and libraries only support 0/1 binary matrices.\nBut, if you think about it for long enough, you‚Äôll realize that we could use this aggregated count data to generate simulated versions of the underlying microdata.\nFor example: notice how this dataset tells us that there are 377 known mammals in Australia, and that of these 377,\n\n41 are Endangered\n9 are Critical,\n57 are Vulnerable, and\n107 are Threatened.\n\nIt relates back to exercises we did early in DSAN 5100, actually: our dataset here provides us with marginal frequencies, but does not give us joint frequencies or conditional frequencies. For example, we don‚Äôt know how many mammals in Australia are both threatened and critical, even though it seems like the number of threatened species is greater than the number of critical species, at least in the rows we can see above.\nIf we did have all of this data, then we could just mathematically derive the number of animals in each possible combination of categories: for example, the exact number of mammals in Canada which are threatened and vulnerable but not endangered or critical. However, since we don‚Äôt know the number of animals in all possible categories (combinations of the four features), we‚Äôre in the same situation as in the simplified example above. So, we‚Äôll have to construct a synthetic dataset, in this case a micro-level dataset of individual animal types which matches the count data in the existing macro-level dataset as best as possible."
  },
  {
    "objectID": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "href": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Approximating the Micro-Level Data",
    "text": "Approximating the Micro-Level Data\nThe key insight is just recognizing that, since we have counts as well as totals in each row, we can construct probabilities of a given animal type having a given property (using the na√Øve definition of probability, and assuming that each property is independent, which is a terrible assumption in this case, but getting rid of this assumption brings us back into scary math world).\nFor example: looking at the four bullet points above, relative to the fact that there are 377 total types of mammals in Australia, we can generate a set of probabilities that a given (arbitrarily-chosen) animal type in Australia has each of these properties. Letting \\(S\\) be a Random Variable representing an animal‚Äôs Species and \\(C\\) a Random Variable representing an animal‚Äôs Country, and then letting \\(\\texttt{M}\\) stand for ‚ÄúMammals‚Äù and \\(\\texttt{A}\\) stand for ‚ÄúAustralia‚Äù:\n\\[\n\\begin{align*}\n\\Pr(\\text{Endangered} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{41}{377} &\\approx 0.109 \\\\\n\\Pr(\\text{Critical} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{9}{377} &\\approx 0.024 \\\\\n\\Pr(\\text{Vulnerable} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{57}{377} &\\approx 0.151 \\\\\n\\Pr(\\text{Threatened} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{107}{377} &\\approx 0.284\n\\end{align*}\n\\]\nWhich means that we could now generate a dataset of individual animals which would approximate (under our very questionable assumptions of independence, at least) the aggregate information we have. For this specific example of mammals in Australia, then, we could write code to generate data for mammals in Australia like the following:\n\nlibrary(Rlab)\nN &lt;- 377\np_endangered &lt;- 0.109\np_critical &lt;- 0.024\np_vulnerable &lt;- 0.151\np_threat &lt;- 0.284\nsynth_animals &lt;- tibble(\n    animal_id = seq(1, N),\n    endangered = rbern(N, p_endangered),\n    critical = rbern(N, p_critical),\n    vulnerable = rbern(N, p_vulnerable),\n    threat = rbern(N, p_threat)\n)\nsynth_animals |&gt; head()\n\n\n\n\n\nanimal_id\nendangered\ncritical\nvulnerable\nthreat\n\n\n\n\n1\n0\n0\n0\n1\n\n\n2\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n1\n\n\n5\n1\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n\n\n\n\n\n\nAnd now we can aggregate these individual ‚Äúcoin flips‚Äù to obtain counts:\n\nagg_animals &lt;- synth_animals |&gt;\n  summarize(\n    ENDANGERED = sum(endangered),\n    CRITICAL = sum(critical),\n    VULNERABLE = sum(vulnerable),\n    THREATENED = sum(threat)\n  ) |&gt;\n  mutate(\n    Source = \"Synthetic\",\n    Country = \"Australia\",\n    Species = \"Mammals\"\n  )\nagg_animals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\nSource\nCountry\nSpecies\n\n\n\n\n37\n6\n65\n92\nSynthetic\nAustralia\nMammals\n\n\n\n\n\n\nAnd we can check how closely they match the macro-level count data:\n\n# First the synthetic data\nagg_animals_long &lt;- agg_animals |&gt; pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n)\n# Now the actual counts\nam_counts_long &lt;- wide_df |&gt;\n  filter(\n    (Species == \"Mammals\") & (Country == \"Australia\")\n  ) |&gt;\n  select(-TOT_KNOWN) |&gt;\n  mutate(Source = \"OECD\") |&gt;\n  pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n  )\n# And combine\nagg_df &lt;- bind_rows(agg_animals_long, am_counts_long)\nagg_df\n\n\n\n\n\nSource\nCountry\nSpecies\nvar\nvalue\n\n\n\n\nSynthetic\nAustralia\nMammals\nENDANGERED\n37\n\n\nSynthetic\nAustralia\nMammals\nCRITICAL\n6\n\n\nSynthetic\nAustralia\nMammals\nVULNERABLE\n65\n\n\nSynthetic\nAustralia\nMammals\nTHREATENED\n92\n\n\nOECD\nAustralia\nMammals\nENDANGERED\n41\n\n\nOECD\nAustralia\nMammals\nCRITICAL\n9\n\n\nOECD\nAustralia\nMammals\nVULNERABLE\n57\n\n\nOECD\nAustralia\nMammals\nTHREATENED\n107\n\n\n\n\n\n\nAnd visualize this closeness:\n\nggplot(agg_df, aes(x=var, y=value, fill=Source)) +\n  geom_bar(stat='identity', position='dodge') +\n  theme_jjdsan() +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)\n  )\n\n\n\n\n\n\n\n\nNot bad! If we repeat this procedure for all (country, species) pairs, we‚Äôll get an individual animal-level (micro-level) dataset! There are several efficient ways to do this, but for the sake of getting it done easily we can just use a loop. However, we‚Äôll have to be careful about missing values."
  },
  {
    "objectID": "posts/synthetic-data/index.html#footnotes",
    "href": "posts/synthetic-data/index.html#footnotes",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe thumbnail image for this post is from MIT Sloan‚Äôs primer on synthetic data.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/images-in-qmd/index.html",
    "href": "posts/images-in-qmd/index.html",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "href": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "href": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "title": "Including Images in .qmd Files",
    "section": "If You‚Äôre Unsure How To Save The Image File Locally",
    "text": "If You‚Äôre Unsure How To Save The Image File Locally\nSometimes, for example, students want to take a photo or scan of a written document, or a screenshot, and include that in a .qmd file. In that case, the difficult part is not knowing the above syntax but knowing how to get the image onto your drive in the first place.\nWhen I want to include screenshots, for example, I tend to use the Mac shortcut cmd+shift+F4cmd+shift+F4, which lets me take a screenshot of some portion of the screen, and then I open the saved screenshot and choose ‚ÄúAdd to Photos‚Äù. Finally, I open up the Photos app and export the image to the images subdirectory mentioned above.\nWhen I want to include photos taken from my phone, there are a lot of different approaches based on different phone types and operating systems, but in general in my case I tend to export the photo to Google Photos or Dropbox, and then download the photo from photos.google.com or dropbox.com and include it in my .qmd file that way."
  },
  {
    "objectID": "posts/poverty-data/index.html",
    "href": "posts/poverty-data/index.html",
    "title": "Visualizing the World Bank‚Äôs Poverty Indicators",
    "section": "",
    "text": "Global Functions and Variables\n\n\n\nHere I‚Äôm just sourcing an R file which is available here. It defines a custom color palette (a pastel-based colorblindness-friendly palette I try to use across all my notes and slides), as well as a custom ggplot theme (before a few additional helper functions). Students, you should feel free to take this and modify it to create your own custom palettes and/or ggplot themes!\n\n\nCode\nsource(\"../../_globals.r\")\n\n\n\n\nGitHub user @light-and-salt has carried out some of the hard work in scraping and combining the World Bank‚Äôs full collection of data on various indicators into a smaller set of simplified, easy-to-use .csv files, so that we can jump right into visualizing them! Here we‚Äôll open up their data on Poverty indicators, specifically.\n\n\nCode\nlibrary(tidyverse)\npoverty_df &lt;- read_csv(\"https://raw.githubusercontent.com/light-and-salt/World-Bank-Data-by-Indicators/master/poverty/poverty.csv\")\npoverty_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nYear\naverage_value_Annualized average growth rate in per capita real survey mean consumption or income, bottom 40% of population (%)\naverage_value_Annualized average growth rate in per capita real survey mean consumption or income, total population (%)\naverage_value_Gini index (World Bank estimate)\naverage_value_Income share held by fourth 20%\naverage_value_Income share held by highest 10%\naverage_value_Income share held by highest 20%\naverage_value_Income share held by lowest 10%\naverage_value_Income share held by lowest 20%\naverage_value_Income share held by second 20%\naverage_value_Income share held by third 20%\naverage_value_Multidimensional poverty headcount ratio (% of total population)\naverage_value_Multidimensional poverty headcount ratio, children (% of population ages 0-17)\naverage_value_Multidimensional poverty headcount ratio, female (% of female population)\naverage_value_Multidimensional poverty headcount ratio, household (% of total households)\naverage_value_Multidimensional poverty headcount ratio, male (% of male population)\naverage_value_Multidimensional poverty index (scale 0-1)\naverage_value_Multidimensional poverty index, children (population ages 0-17) (scale 0-1)\naverage_value_Multidimensional poverty intensity (average share of deprivations experienced by the poor)\naverage_value_Population living in slums (% of urban population)\naverage_value_Poverty gap at $1.90 a day (2011 PPP) (%)\naverage_value_Poverty gap at $3.20 a day (2011 PPP) (%)\naverage_value_Poverty gap at $5.50 a day (2011 PPP) (%)\naverage_value_Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\naverage_value_Poverty headcount ratio at $3.20 a day (2011 PPP) (% of population)\naverage_value_Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)\naverage_value_Poverty headcount ratio at national poverty lines (% of population)\naverage_value_Proportion of people living below 50 percent of median income (%)\naverage_value_Survey mean consumption or income per capita, bottom 40% of population (2011 PPP $ per day)\naverage_value_Survey mean consumption or income per capita, total population (2011 PPP $ per day)\n\n\n\n\nCentral African Republic\nCAF\n1992\nNA\nNA\n61.3\n18.5\n47.7\n65.0\n0.7\n2.0\n4.9\n9.6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n58.6\n70.9\n81.0\n84.1\n92.2\n96.7\nNA\n29.1\nNA\nNA\n\n\nChina\nCHN\n2014\nNA\nNA\n39.2\n22.3\n29.7\n45.8\n2.5\n6.2\n10.5\n15.2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n25.2\n0.3\n2.1\n9.7\n1.4\n9.5\n31.5\n7.2\nNA\nNA\nNA\n\n\nFiji\nFJI\n2008\nNA\nNA\n40.4\n20.5\n32.9\n47.8\n2.7\n6.6\n10.8\n14.4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0.2\n2.3\n11.1\n1.6\n10.9\n37.3\n35.2\n11.7\nNA\nNA\n\n\nGambia, The\nGMB\n2003\nNA\nNA\n47.3\n20.7\n36.8\n52.8\n1.8\n4.8\n8.7\n13.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n17.9\n34.7\n53.8\n46.1\n69.8\n87.3\nNA\n17.4\nNA\nNA\n\n\nHaiti\nHTI\n2010\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nIndia\nIND\n2005\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n34.8\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nSo, although the rows don‚Äôt seem to be in any particular order, we can see to our relief that this data is tidy:\n\nEach row represents a unit of observation (in this case, a country-year pair),\nEach column represents a property of the unit of observation, and\nEach cell‚Äîfor example, a cell in row \\(i\\) and column \\(j\\)‚Äîrepresents the value of property \\(j\\) for unit of observation \\(i\\).\n\nWe can look at the column headers to see how we can start visualizing the properties we have: aside from the id variables Country Name, Country Code, and Year, all of the columns have the form average_value_&lt;variable description&gt;, where &lt;variable description&gt; is the short description given for each variable within the original World Bank data.\nAlthough these variables represent a wide range of different ways to measure poverty (for example, it‚Äôs difficult to choose a priori between $5.50, $3.20, and $1.90 as the ‚Äúcorrect‚Äù threshold for poverty), the one that seems to require a relatively low amount of context is:\naverage_value_Proportion of people living below 50 percent of median income (%)\nSo, let‚Äôs use the rename() function from tidyverse to make this long name a bit more manageable, then the select() function to extract just this variable (alongside the id variables):\n\n\nCode\nmedian_df &lt;- poverty_df |&gt; rename(\n    below_half_median = `average_value_Proportion of people living below 50 percent of median income (%)`,\n    country = `Country Name`\n) |&gt; select(country, `Year`, below_half_median)\nmedian_df |&gt; head()\n\n\n\n\n\n\ncountry\nYear\nbelow_half_median\n\n\n\n\nCentral African Republic\n1992\n29.1\n\n\nChina\n2014\nNA\n\n\nFiji\n2008\n11.7\n\n\nGambia, The\n2003\n17.4\n\n\nHaiti\n2010\nNA\n\n\nIndia\n2005\nNA\n\n\n\n\n\n\nNext, as one additional simplifying step, let‚Äôs convert this country x year (panel) dataset into just a country (cross-sectional) dataset, by computing the mean value for a given country‚Äôs measures across all of the years in which the country measure is available. I‚Äôll also make a column indicating how many years were used to compute this mean, so that we can drop countries with too few years, if we‚Äôd like:\n\n\nCode\nsumm_df &lt;- median_df |&gt; group_by(country) |&gt;\n  summarize(\n    below_half_med_avg = mean(below_half_median),\n    year_count = n()\n  ) |&gt; drop_na()\nsumm_df |&gt; head()\n\n\n\n\n\n\ncountry\nbelow_half_med_avg\nyear_count\n\n\n\n\nAlbania\n10.033333\n9\n\n\nAlgeria\n11.466667\n3\n\n\nArmenia\n7.655000\n20\n\n\nAustralia\n10.320000\n10\n\n\nBelarus\n8.140909\n22\n\n\nBotswana\n22.600000\n5\n\n\n\n\n\n\nSince it may be unwieldy to plot all ~200 countries in the world, let‚Äôs make our visualizations a bit more digestible by focusing in on just the 10 countries with the highest poverty levels and the 10 with the lowest poverty levels. We‚Äôll start by extracting the 10 countries with the highest levels (adding a column named category to keep track of the fact that we‚Äôve extracted these countries as having the highest poverty rates, for later when we‚Äôll plot the highest and lowest together):\n\n\nCode\nhighest_df &lt;- summ_df |&gt;\n  arrange(desc(below_half_med_avg)) |&gt;\n  head(10) |&gt;\n  mutate(Category = \"10 Highest\")\nhighest_df |&gt; arrange(below_half_med_avg)\n\n\n\n\n\n\ncountry\nbelow_half_med_avg\nyear_count\nCategory\n\n\n\n\nUnited States\n17.77742\n31\n10 Highest\n\n\nUruguay\n18.66400\n25\n10 Highest\n\n\nPapua New Guinea\n19.20000\n2\n10 Highest\n\n\nIsrael\n19.26364\n11\n10 Highest\n\n\nCosta Rica\n20.06364\n33\n10 Highest\n\n\nEcuador\n20.65000\n20\n10 Highest\n\n\nParaguay\n21.12609\n23\n10 Highest\n\n\nBotswana\n22.60000\n5\n10 Highest\n\n\nPanama\n24.97407\n27\n10 Highest\n\n\nHonduras\n25.07000\n30\n10 Highest\n\n\n\n\n\n\nAnd now, finally, we have our first visualization choice: what is a reasonable way to visualize these data points, and why should we pick this way over alternative ways to visualize it? I would argue, first, that since the country variable is categorical and non-ordinal, but the below_half_med_avg variable is continuous, we should represent this using some sort of bar-chart-esque method.\nHere I just want to point out: when we use bar charts, one tricky consideration is the fact that these types of charts implicitly draw one‚Äôs attention to the zero point (the ‚Äúbottom‚Äù of the bar), since the lines are proportional to the distance of each number from zero. Later on we‚Äôll address this explicitly by plotting relative to the global mean instead of zero, but for now I will just draw your attention to this factor and use a ‚Äúlollipop‚Äù plot instead of a bar chart, to draw the viewer‚Äôs attention to the points themeslves and de-emphasize the bars (by narrowing them down to the size of lines):\n\n\nCode\n# I'm also defining string variables that we can use\n# as titles/labels for the remainder of the analysis\ntitle &lt;- \"Poverty Rate by Country\"\ncat_label &lt;- \"Country\"\nrate_label &lt;- \"Percent Under 1/2 of Median Income\"\nhighest_df |&gt;\n  mutate(country = fct_reorder(country, below_half_med_avg)) |&gt;\n  ggplot(aes(y=below_half_med_avg, x=country)) +\n  geom_point() +\n  geom_segment(aes(xend=country, yend=0)) +\n  coord_flip() +\n  #theme_classic() +\n  theme_jjdsan() +\n  labs(title=title, y=rate_label, x=cat_label)\n\n\n\n\n\n\n\n\n\nNow let‚Äôs repeat these steps, but for the 10 countries with the lowest poverty rates:\n\n\nCode\nlowest_df &lt;- summ_df |&gt;\n  arrange(below_half_med_avg) |&gt;\n  head(10) |&gt;\n  mutate(Category = \"10 Lowest\")\nlowest_df\n\n\n\n\n\n\ncountry\nbelow_half_med_avg\nyear_count\nCategory\n\n\n\n\nKyrgyz Republic\n6.185000\n20\n10 Lowest\n\n\nUkraine\n6.922727\n22\n10 Lowest\n\n\nKazakhstan\n7.347368\n19\n10 Lowest\n\n\nArmenia\n7.655000\n20\n10 Lowest\n\n\nKosovo\n7.783333\n12\n10 Lowest\n\n\nBelarus\n8.140909\n22\n10 Lowest\n\n\nSri Lanka\n8.350000\n8\n10 Lowest\n\n\nMoldova\n10.027273\n22\n10 Lowest\n\n\nAlbania\n10.033333\n9\n10 Lowest\n\n\nAustralia\n10.320000\n10\n10 Lowest\n\n\n\n\n\n\n\n\nCode\nlowest_df |&gt;\n  mutate(country = fct_reorder(country, below_half_med_avg)) |&gt;\n  ggplot(aes(y=below_half_med_avg, x=country)) +\n  geom_point() +\n  geom_segment(aes(xend=country, yend=0)) +\n  coord_flip() +\n  #theme_classic() +\n  theme_jjdsan() +\n  labs(title=title, y=rate_label, x=cat_label)\n\n\n\n\n\n\n\n\n\nNotice how, if we just eyeball these two plots without paying close attention, they don‚Äôt look very different. In fact, the hurried viewer may just look at the ‚Äúcurve‚Äù formed by the ends of the lines, conclude that they convey the same information, and move on. This happens, in large part, because the x-axis scales of the two plots are different, despite the fact that they are measuring the exact same variable. To ensure that it‚Äôs clear to the viewer that these two plots are displaying values of the same variable (poverty rate) on their x-axes, let‚Äôs plot the top 10 and bottom 10 together in the same figure:\n\n\nCode\nhigh_low_df &lt;- bind_rows(highest_df, lowest_df)\nhigh_low_df |&gt;\n  mutate(country = fct_reorder(country, below_half_med_avg)) |&gt;\n  ggplot(aes(y=below_half_med_avg, x=country)) +\n  geom_point() +\n  geom_segment(aes(xend=country, yend=0)) +\n  coord_flip() +\n  #theme_classic() +\n  theme_jjdsan() +\n  labs(title=title, y=rate_label, x=cat_label)\n\n\n\n\n\n\n\n\n\nIn the figure we just generated, we already have a fairly extreme example of the dangers that can come from presenting charts without proper context: since the audience doesn‚Äôt know that we‚Äôve extracted only the top 10 and bottom 10 countries, they may reasonably look at the figure and infer (for example) that perhaps there is a large drop between the 10th-ranked and 11th-ranked countries in terms of poverty rate, for some reason.\nAs a potential ‚Äúquick fix‚Äù, and to show how we can keep changing what the data ‚Äúsays‚Äù even in this simple example, let‚Äôs plot the 10 countries with the highest poverty rate (on this measure) in a shade of red, and the 10 countries with the lowest rates in a shade of green, and plot all 20 relative to the median rate across all countries and years:\n\n\nCode\nhigh_low_df |&gt;\n  mutate(country = fct_reorder(country, below_half_med_avg)) |&gt;\n  ggplot(aes(y=below_half_med_avg, x=country, color=Category)) +\n  geom_point() +\n  geom_segment(aes(xend=country, yend=0)) +\n  coord_flip() +\n  #theme_classic() +\n  theme_jjdsan() +\n  labs(title=title, y=rate_label, x=cat_label)\n\n\n\n\n\n\n\n\n\nNow, I still find this presentation fairly misleading in many ways, but especially in the sense that it makes the value of zero seem more relevant than it actually is. These countries, for example, were not selected on the basis of their difference from zero as such, but instead on the basis of how extreme they are relative to the average of all the countries. So, for plotting each country‚Äôs poverty rate relative to this global mean, we can compute and save a global_mean variable from summ_df:\n\n\nCode\n(global_mean &lt;- mean(summ_df$below_half_med_avg))\n\n\n[1] 13.97053\n\n\nAnd now we can generate a new plot, where this comparison with the mean country (rather than the more arbitrary/unjustified comparison with the zero point) is made explicit via a dashed vertical line:\n\n\nCode\nhigh_low_df |&gt;\n  mutate(country = fct_reorder(country, below_half_med_avg)) |&gt;\n  ggplot(aes(y=below_half_med_avg, x=country, color=Category)) +\n  geom_point() +\n  geom_segment(aes(xend=country, yend=global_mean)) +\n  geom_hline(yintercept=global_mean, linetype=\"dashed\") +\n  coord_flip() +\n  #theme_classic() +\n  theme_jjdsan() +\n  labs(title=title, y=rate_label, x=cat_label)\n\n\n\n\n\n\n\n\n\nThere is a lot more we can do with this dataset! So you can keep it in mind as we discuss more and learn more advanced visualization techniques. For now, and for inspiration/examples, you can check out Nathan Yau‚Äôs blog post One Dataset, Visualized 25 Ways, which is what I had in the back of my mind when making this demo!"
  },
  {
    "objectID": "posts/liwc/index.html",
    "href": "posts/liwc/index.html",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#the-text-files",
    "href": "posts/liwc/index.html#the-text-files",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "href": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Converting Into Python Regular Expressions",
    "text": "Converting Into Python Regular Expressions\nUsing them in this raw format is a bit tricky, however, since they are formatted not as individual words but as regular expressions, which will match entire families of positive and negative words. For example, 032-negemo.txt contains the entry troubl*, which will therefore match the words trouble, troubles, troubling, and so on.\nSo, to work with these files in Python, we‚Äôll need to load the .txt files but then convert each entry into a regular expression object. This can be done using the following collection of functions:\n\nimport re\ndef load_liwc_list(filepath):\n    \"\"\"\n    :return: A list of words loaded from the file at `fpath`\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        words = infile.read().split()\n    return words\n\ndef liwc_to_regex(liwc_list):\n    \"\"\"\n    Converts LIWC expression list into Python regular expression\n    \"\"\"\n    wildcard_reg = [w.replace('*', r'[^\\s]*') for w in liwc_list]\n    reg_str = r'\\b(' + '|'.join(wildcard_reg) + r')\\b'\n    return reg_str\n\ndef num_matches(reg_str, test_str):\n    num_matches = len(re.findall(reg_str,test_str))\n    return num_matches\n\ndef file_to_regex(filepath):\n    liwc_list = load_liwc_list(filepath)\n    liwc_regex_list = liwc_to_regex(liwc_list)\n    return liwc_regex_list\n\n# You can call the following helper function if\n# you'd like to see the full regular expression\ndef print_regex(regex_str, wrap_col=70):\n    import textwrap\n    print(textwrap.fill(regex_str, wrap_col))\n\nWe can use these functions to load the .txt files and create lists of regex (Regular Expression) objects from them:\n\npos_fpath = \"./assets/liwc/031-posemo.txt\"\npos_regex = file_to_regex(pos_fpath)\nneg_fpath = \"./assets/liwc/032-negemo.txt\"\nneg_regex = file_to_regex(neg_fpath)\n# Uncomment this line to see the full regular expression\n#print_regex(neg_regex)\nprint_regex(neg_regex[:140])\n\n\\b(dismay[^\\s]*|ignorant|poorest|tragic|disreput[^\\s]*|ignore|poorly|t\nrauma[^\\s]*|abandon[^\\s]*|diss|ignored|poorness[^\\s]*|trembl[^\\s]*|abu"
  },
  {
    "objectID": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "href": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Using the Regular Expressions to Generate Sentiment Scores",
    "text": "Using the Regular Expressions to Generate Sentiment Scores\nAnd now we can use these generated regular expressions to count the number of times ‚Äúpositive‚Äù and ‚Äúnegative‚Äù words appear in our string! Here we provide two final helper functions for accomplishing this:\n\ndef extract_sentiment_data(text):\n    # First compute positive sentiment using pos_reg\n    pos_count = num_matches(pos_regex, text)\n    # Then negative sentiment using neg_reg\n    neg_count = num_matches(neg_regex, text)\n    # And finally the overall sentiment score as the difference\n    sentiment = pos_count - neg_count\n    return {\n        'pos': pos_count,\n        'neg': neg_count,\n        'sentiment': sentiment\n    }\n\ndef compute_sentiment(text):\n    full_results = extract_sentiment_data(text)\n    # Return just the overall sentiment score\n    return full_results['sentiment']\n\nAnd here we test these helper functions out by creating positive, negative, and neutral test strings and checking the results for these strings:\n\nneg_test_str = \"Python is terrible, I hate Python, I despise Python\"\nneg_str_results = extract_sentiment_data(neg_test_str)\nprint(f\"{neg_test_str}\\n{neg_str_results}\")\npos_test_str = \"Python is wonderful, I love Python, I adore Python\"\npos_str_results = extract_sentiment_data(pos_test_str)\nprint(f\"{pos_test_str}\\n{pos_str_results}\")\nneutral_test_str = \"Python is ok, Python is mid, I guess I can do Python maybe\"\nneutral_str_results = extract_sentiment_data(neutral_test_str)\nprint(f\"{neutral_test_str}\\n{neutral_str_results}\")\n\nPython is terrible, I hate Python, I despise Python\n{'pos': 0, 'neg': 3, 'sentiment': -3}\nPython is wonderful, I love Python, I adore Python\n{'pos': 3, 'neg': 0, 'sentiment': 3}\nPython is ok, Python is mid, I guess I can do Python maybe\n{'pos': 1, 'neg': 0, 'sentiment': 1}"
  },
  {
    "objectID": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "href": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Computing Sentiment Scores for a DataFrame Column",
    "text": "Computing Sentiment Scores for a DataFrame Column\nEven though above we printed out the full results of each sentiment computation (by calling extract_sentiment_data(), which returns a dictionary containing the results), if you have a DataFrame with a text column that you‚Äôd like to perform sentiment analysis on, you can just use the simpler compute_sentiment() function to obtain a single number, like in the following code:\n\nimport pandas as pd\ntext_df = pd.DataFrame({\n    'text_id': [1,2,3],\n    'text': [neg_test_str, pos_test_str, neutral_test_str]\n})\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n\n\n\n\n\n\n\n\ntext_df['sentiment'] = text_df['text'].apply(compute_sentiment)\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\nsentiment\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n-3\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n3\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n1"
  },
  {
    "objectID": "posts/liwc/index.html#footnotes",
    "href": "posts/liwc/index.html#footnotes",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd, in case you want to use it in the future, you can download the entire set of word lists as a zip file here‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/datasets-for-viz/index.html",
    "href": "posts/datasets-for-viz/index.html",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "",
    "text": "There are lots of datasets out there, to the point that it can be scary trawling through e.g.¬†the (at time of writing) 1,780 datasets listed in the Data is Plural archive. This point of this post, therefore, is to provide a ‚Äúcurated‚Äù collection of datasets specifically chosen for how well they lend themselves to various methods of visualization. For the section of DSAN 5200: Advanced Data Visualization that I‚Äôm teaching this semester (Section 03), I‚Äôll be drawing on these datasets for demonstrations of how we can produce visual representations from raw numeric or text data."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#life-expectancy-by-country",
    "href": "posts/datasets-for-viz/index.html#life-expectancy-by-country",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Life Expectancy By Country",
    "text": "Life Expectancy By Country\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Left: The life expectancy gap between males and females in 2015, by country. Right: The change in life expectancy between 2000 and 2015, by country. Both from Nathan Yau‚Äôs One Dataset, Visualized 25 Ways.\n\n\n\nThe data, from the World Health Organization‚Äôs Global Health Observatory.\n\nType: Panel (Country x Year)\n\nThis is the dataset behind Nathan Yau‚Äôs blog post One Dataset, Visualized 25 Ways. This post provides a great example of how data doesn‚Äôt ‚Äúspeak for itself‚Äù, and how visualization choices‚Äîwhat aspects to highlight and what aspects to exclude‚Äîcan have a big impact on how underlying patterns in the data are extracted, communicated, and understood by an audience."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#perception-of-probability-words",
    "href": "posts/datasets-for-viz/index.html#perception-of-probability-words",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Perception of Probability Words",
    "text": "Perception of Probability Words\n\n\n\n\n\n\nFigure¬†2: Joyplot of the association between phrases and numeric values, from GitHub repo of reddit user zonination\n\n\n\n\nType: Cross-sectional (one row per respondent)\nSurvey of 123 redditors, based on a set of charts from an earlier (since-declassified) CIA publication on ‚ÄúWords of Estimative Probability‚Äù, data here\nVisualization of a precursor to this dataset on D3 Graph Gallery\nVisualization of this data, along with additional contextual details, from Illinois Computer Science professor Wade Fagen-Ulmschneider\nAnalysis from Visual Capitalist\n\nThis is one of my favorite datasets for visualization, and just for pondering in general, because it brings out the tension between the ‚Äúfuzziness‚Äù of human decision-making and the numeric precision which is required for the algorithmic decision-making carried out by computers. Thinking through the visualizations of the above data on this page ‚Äúin reverse‚Äù can help us understand how humans perceive the outputs of probabilistic machine learning algorithms: there is a sense in which an ‚Äúaverage‚Äù human (generalizing speculatively from this small-\\(N\\) survey!) will interpret an algorithm‚Äôs output of 0.95 as an ‚Äúalmost certain‚Äù result, while a result between 0.15 and 0.30 will be interpreted by ~50% of humans as a ‚Äúprobably not‚Äù result."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#banknote-faces",
    "href": "posts/datasets-for-viz/index.html#banknote-faces",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Banknote Faces",
    "text": "Banknote Faces\n\n\n\n\n\n\nFigure¬†3: From the pudding.cool article Who‚Äôs In Your Wallet?, visualizing the gaps between when a public figure died and when their face was put on a country‚Äôs banknotes.\n\n\n\n\nType: Cross-sectional (one row per person)\nData available via the GitHub repo for the article\n\npudding.cool is one of my favorite websites of all time, and has dozens and dozens of fascinating datasets and analyses, but this one I think is really straightforward yet lends itself to lots of different visualizations, like the one presented above plotting the gaps in time between when a person died and when their face was placed on a country‚Äôs banknotes. One of my favorite aspects about this plot is how it leads the viewer (and by the viewer I specifically mean me! But, I would guess that others may be intrigued by this aspect as well?) to want to know what the ‚Äúcorner cases‚Äù represent: who is the one person who died ~2000 years before they were placed on a bill? Or, who are the 10 people who were still alive when their face was placed on a banknote? I guess you‚Äôll have to go in and explore the data (or read the pudding.cool article) to find out! (Or click the text below to reveal the answers, if you want to cheat üòú)\n\n\n(Click for the gist if you don‚Äôt want to read the article!)\n\n\nThe outlier in terms of dying ~2000 years before being placed on a bill is Hannibal, who died around 182 BC and was placed on the 5 Tunisian Dinar bill in 2013.\nThe ten outliers who lived to see their own faces on a banknote are (in chronological order of banknote issue):\n\nMustafa Kemal Atat√ºrk, placed on Turkish Lira note in 1927 and died in 1938\nSukarno, placed on Indonesian Rupiah note in 1945 and died in 1970\nMichael Manley of Jamaica, placed on Jamaican Dollar note in 1970 and died in 1997\nHastings Banda of Malawi, placed on Malawian Kwacha note in 1971 and died in 1997\nSheikh Mujibur Rahman, placed on Bangladeshi Taka note in 1972 and died in 1975\nMichael Somare, placed on Papua New Guinean Kina note in 1989, died in 2021\nEdmund Hillary, placed on New Zealand Dollar note in 1992, died in 2008\nRose Chibambo, placed on Malawian Kwacha note in 2012, died in 2016\nNelson Mandela, placed on South African Rand note in 2012, died in 2013\nQueen Elizabth II of England, placed on British Pound note in 1963, died in 2022"
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#the-global-flavor-map",
    "href": "posts/datasets-for-viz/index.html#the-global-flavor-map",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "The Global Flavor Map",
    "text": "The Global Flavor Map\n\n\n\n\n\n\nFigure¬†4: A high-level view of the ‚Äúglobal flavor network‚Äù developed and studied in (ahn_flavor_2011?)\n\n\n\n\nType: Network data (Nodes are ingredients, edges between two nodes represent shared molecular properties)\nThe data: available at the bottom of the page for the paper, as ‚ÄúSupplementary Dataset 1‚Äù\n\nIt‚Äôs difficult to choose a good dataset to demonstrate visualization of network data, honestly, since sometimes ‚Äúgood‚Äù means that it is simple enough to capture using just a few nodes and a few edges, while other times ‚Äúgood‚Äù means that it is rich/complex enough to warrant generating a bunch of different visualizations showing different facets of the data. I think this dataset falls more into the latter case, but I like it in terms of generating the visualization reproduced above, since it shows how you could generate simple ‚Äúsummary‚Äù visualizations of the dataset, but (as done in the paper from which the figure is taken) you can also do a deep dive into the dataset‚Äîdown into the molecular level, for example‚Äîto find other interesting patterns that too ‚Äúfine grained‚Äù to be present in the above (high-level/coarse-grained) visualization."
  },
  {
    "objectID": "posts/large-files/index.html",
    "href": "posts/large-files/index.html",
    "title": "Handling Large Files in Projects",
    "section": "",
    "text": "Since final projects in DSAN span a wide set of domains/application areas, and therefore a wide range of dataset structures, it‚Äôs common to have files larger than the 50 MB recommended maximum (and 100 MB absolute maximum) imposed by GitHub!\nIn these cases, at least in the research domains I‚Äôve worked in, the most common approach is to:\n\nStore your code and small data files on GitHub, but\nStore your large data files in a cloud storage service like Google Drive\n\nIntegrating the larger data files into your code, then, becomes the hard part. Since Georgetown students have a large Google Drive allocation built into your @georgetown.edu accounts, here I will show how to store large data files in Google Drive and then load them into your code, in both R and Python."
  },
  {
    "objectID": "posts/large-files/index.html#the-problem",
    "href": "posts/large-files/index.html#the-problem",
    "title": "Handling Large Files in Projects",
    "section": "",
    "text": "Since final projects in DSAN span a wide set of domains/application areas, and therefore a wide range of dataset structures, it‚Äôs common to have files larger than the 50 MB recommended maximum (and 100 MB absolute maximum) imposed by GitHub!\nIn these cases, at least in the research domains I‚Äôve worked in, the most common approach is to:\n\nStore your code and small data files on GitHub, but\nStore your large data files in a cloud storage service like Google Drive\n\nIntegrating the larger data files into your code, then, becomes the hard part. Since Georgetown students have a large Google Drive allocation built into your @georgetown.edu accounts, here I will show how to store large data files in Google Drive and then load them into your code, in both R and Python."
  },
  {
    "objectID": "posts/large-files/index.html#uploading-files-and-setting-permissions",
    "href": "posts/large-files/index.html#uploading-files-and-setting-permissions",
    "title": "Handling Large Files in Projects",
    "section": "Uploading Files and Setting Permissions",
    "text": "Uploading Files and Setting Permissions\nThe first step is to upload your large data file(s) to a folder in Google Drive, then right-click on the file(s) you‚Äôd like to incorporate into your code and click ‚ÄúShare‚Äù, then set permissions to ‚ÄúAnyone with the link‚Äù.\nThe example I‚Äôm using here is a US county-level dataset on outcomes from the Opportunity Atlas. It contains 10827 attributes for 3219 counties, hence taking up about 184 MB and putting it over GitHub‚Äôs 100 MB limit.\nSo, once I set the permissions as described, I am given the following sharing link:\nhttps://drive.google.com/file/d/1q8LHQ8Etdd1aYYChLZ94ujJ4GAOd6s2g/view?usp=drive_link"
  },
  {
    "objectID": "posts/large-files/index.html#converting-to-a-direct-download-link",
    "href": "posts/large-files/index.html#converting-to-a-direct-download-link",
    "title": "Handling Large Files in Projects",
    "section": "Converting to a Direct-Download Link",
    "text": "Converting to a Direct-Download Link\nIf you click that link, however, you‚Äôll see that it is not a direct link to the county_outcomes.csv file itself, but a link to a ‚Äúwrapper‚Äù page showing info about the file, with a link you can manually click to then download the file itself. This link as-is, therefore, will not be very helpful for us in terms of writing code to load this dataset the same way we‚Äôd load a local data file.\nA not-very-well-known (but crucially important for our purposes!) aspect of Google Drive is that you can convert this ‚Äúviewing‚Äù link into a direct-download link! If our files were less than 100 MB, we could use this direct-download link as-is, plugging it into pd.read_csv() (in Python) or read_csv() in R‚Äôs tidyverse."
  },
  {
    "objectID": "posts/large-files/index.html#bypassing-the-large-file-warning-with-gdown",
    "href": "posts/large-files/index.html#bypassing-the-large-file-warning-with-gdown",
    "title": "Handling Large Files in Projects",
    "section": "Bypassing the Large-File Warning with gdown",
    "text": "Bypassing the Large-File Warning with gdown\nUnfortunately for us, however, when files are over 100 MB the direct-download link comes with a ‚Äúwarning screen‚Äù, telling users that the file is ‚Äútoo big to scan for viruses‚Äù, and asking if they would like to proceed. You can see this warning screen if you click the following direct-download link for the file we‚Äôre looking at:\nhttps://drive.google.com/uc?export=download&id=1q8LHQ8Etdd1aYYChLZ94ujJ4GAOd6s2g\nSo, the final missing piece to bypass this warning screen and actually direct-download the file into Python or R comes from the gdown library! I‚Äôve made the following Colab notebooks to walk you through how to use this library in either language to directly incorporate this large .csv file! (In Python we can use it directly, since it‚Äôs written in Python and installable using pip. In R, we can still use it by programmatically executing the gdown terminal command using R‚Äôs system() function. See the notebooks for more!)\n\nColab Notebook: Large Files in Python\nColab Notebook: Large Files in R"
  },
  {
    "objectID": "posts/mounting-gu-domains/index.html",
    "href": "posts/mounting-gu-domains/index.html",
    "title": "Mounting Your GU Domains Space as a Local Drive",
    "section": "",
    "text": "You can use a Mac App called Mountain Duck to mount your GU Domains space as a disk on your local computer, so that you can upload/download/view/modify files on your GU Domains space the same way you work with files on your local computer.\nOn my Mac, for example, I have my GU Domains server space mounted so that my Finder sidebar looks as follows:\n\n\n\n\n\nSo that when I open this Georgetown Domains ‚Äúdrive‚Äù, I see the following:"
  }
]