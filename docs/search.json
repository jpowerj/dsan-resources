[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources for DSAN Students",
    "section": "",
    "text": "This page contains some general resources for DSAN students, which I hope can be useful across multiple different DSAN courses and/or in your future careers! ü§©\n\n\n\n\n\n\n\n\n  \n\n\n\n\nJeff‚Äôs Big Bucket of Datasets for Visualization\n\n\n\n\n\n\n\nDatasets\n\n\n\n\n\n\n\n\n\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Synthetic Microdata from Aggregated Count Data\n\n\n\n\n\n\n\nGeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nUsing LIWC for Document-Level Sentiment Analysis\n\n\n\n\n\n\n\nText Analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nEstablishing Global Settings for R in VSCode\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nKangheng Liu, Jeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nMounting Your GU Domains Space as a Local Drive\n\n\n\n\n\n\n\nGU Domains\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nIncluding Images in .qmd Files\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rprofile-globals/index.html",
    "href": "posts/rprofile-globals/index.html",
    "title": "Establishing Global Settings for R in VSCode",
    "section": "",
    "text": "Awesome student Kangheng Liu figured out the following, which will be helpful for students who use the dark (default!) theme for VSCode but find that their R plots show up as black labels on a black background, so that they are not readable:\nThe IRkernel actually calls the R default startup process. The .Rprofile works whether inside or outside of jupyter.\nThe problem was R‚Äôs logic in sourcing the .Rprofile file. It won‚Äôt load libraries/packages in starting R kernel process. To pass background color as white, we need to write a hook for that. Search for hook in the link for explanations.\nThe coding is as below. in .Rprofile at home folder (or project folder, will take presendence if any):\nsetHook(packageEvent(\"grDevices\", \"onLoad\"),\n        function(...) grDevices::pdf.options(bg=\"white\"))\nThis will pass bg=\"white\" to grDevices, which is the R graphics device package.\nThank you Kangheng, on behalf of the DSAN students, for discovering this!"
  },
  {
    "objectID": "posts/synthetic-data/index.html",
    "href": "posts/synthetic-data/index.html",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "href": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-count-data",
    "href": "posts/synthetic-data/index.html#macro-level-count-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Macro-Level Count Data",
    "text": "Macro-Level Count Data\nSo, say you have a dataset that looks like the following, for example, obtained from the OECD‚Äôs Data Explorer portal1:\n\nlibrary(tidyverse)\nmacro_df &lt;- read_csv(\"assets/wild_life_cleaned.csv\")\nmacro_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIUCN\nIUCN Category\nSPEC\nSpecies\nCOU\nCountry\nValue\n\n\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUS\nAustralia\n377\n\n\nENDANGERED\nNumber of endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n41\n\n\nCRITICAL\nNumber of critically endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n9\n\n\nVULNERABLE\nNumber of vulnerable species\nMAMMAL\nMammals\nAUS\nAustralia\n57\n\n\nTHREATENED\nTotal number of threatened species\nMAMMAL\nMammals\nAUS\nAustralia\n107\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUT\nAustria\n104\n\n\n\n\n\n\nOnce we transform it from long to wide format, we‚Äôll be able to see how this dataset provides data on (country-species) pairs (e.g., Mammals in Australia), not on individual types of animals (e.g., Kangaroos in Australia):\n\nwide_df &lt;- macro_df |&gt;\n  select(-c(`IUCN Category`, COU, SPEC)) |&gt;\n  pivot_wider(\n    names_from = IUCN,\n    values_from = Value\n  ) |&gt;\n  select(-c(THREAT_PERCENT))\nwide_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nCountry\nTOT_KNOWN\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\n\n\n\n\nMammals\nAustralia\n377\n41\n9\n57\n107\n\n\nMammals\nAustria\n104\n6\n4\n17\n27\n\n\nMammals\nBelgium\n84\n2\n4\n12\n18\n\n\nMammals\nCanada\n222\n11\n11\n33\n55\n\n\nMammals\nCzech Republic\n92\n1\n10\n4\n15\n\n\nMammals\nDenmark\n71\n2\n1\n6\n9\n\n\n\n\n\n\nAlthough some methods like Decision Trees can handle both classification and regression tasks, there are other methods like ARM where using this type of count data can be difficult, since most of the methods and libraries only support 0/1 binary matrices.\nBut, if you think about it for long enough, you‚Äôll realize that we could use this aggregated count data to generate simulated versions of the underlying microdata.\nFor example: notice how this dataset tells us that there are 377 known mammals in Australia, and that of these 377,\n\n41 are Endangered\n9 are Critical,\n57 are Vulnerable, and\n107 are Threatened.\n\nIt relates back to exercises we did early in DSAN 5100, actually: our dataset here provides us with marginal frequencies, but does not give us joint frequencies or conditional frequencies. For example, we don‚Äôt know how many mammals in Australia are both threatened and critical, even though it seems like the number of threatened species is greater than the number of critical species, at least in the rows we can see above.\nIf we did have all of this data, then we could just mathematically derive the number of animals in each possible combination of categories: for example, the exact number of mammals in Canada which are threatened and vulnerable but not endangered or critical. However, since we don‚Äôt know the number of animals in all possible categories (combinations of the four features), we‚Äôre in the same situation as in the simplified example above. So, we‚Äôll have to construct a synthetic dataset, in this case a micro-level dataset of individual animal types which matches the count data in the existing macro-level dataset as best as possible."
  },
  {
    "objectID": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "href": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Approximating the Micro-Level Data",
    "text": "Approximating the Micro-Level Data\nThe key insight is just recognizing that, since we have counts as well as totals in each row, we can construct probabilities of a given animal type having a given property (using the na√Øve definition of probability, and assuming that each property is independent, which is a terrible assumption in this case, but getting rid of this assumption brings us back into scary math world).\nFor example: looking at the four bullet points above, relative to the fact that there are 377 total types of mammals in Australia, we can generate a set of probabilities that a given (arbitrarily-chosen) animal type in Australia has each of these properties. Letting \\(S\\) be a Random Variable representing an animal‚Äôs Species and \\(C\\) a Random Variable representing an animal‚Äôs Country, and then letting \\(\\texttt{M}\\) stand for ‚ÄúMammals‚Äù and \\(\\texttt{A}\\) stand for ‚ÄúAustralia‚Äù:\n\\[\n\\begin{align*}\n\\Pr(\\text{Endangered} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{41}{377} &\\approx 0.109 \\\\\n\\Pr(\\text{Critical} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{9}{377} &\\approx 0.024 \\\\\n\\Pr(\\text{Vulnerable} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{57}{377} &\\approx 0.151 \\\\\n\\Pr(\\text{Threatened} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{107}{377} &\\approx 0.284\n\\end{align*}\n\\]\nWhich means that we could now generate a dataset of individual animals which would approximate (under our very questionable assumptions of independence, at least) the aggregate information we have. For this specific example of mammals in Australia, then, we could write code to generate data for mammals in Australia like the following:\n\nlibrary(Rlab)\nN &lt;- 377\np_endangered &lt;- 0.109\np_critical &lt;- 0.024\np_vulnerable &lt;- 0.151\np_threat &lt;- 0.284\nsynth_animals &lt;- tibble(\n    animal_id = seq(1, N),\n    endangered = rbern(N, p_endangered),\n    critical = rbern(N, p_critical),\n    vulnerable = rbern(N, p_vulnerable),\n    threat = rbern(N, p_threat)\n)\nsynth_animals |&gt; head()\n\n\n\n\n\nanimal_id\nendangered\ncritical\nvulnerable\nthreat\n\n\n\n\n1\n0\n0\n0\n1\n\n\n2\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n1\n\n\n5\n1\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n\n\n\n\n\n\nAnd now we can aggregate these individual ‚Äúcoin flips‚Äù to obtain counts:\n\nagg_animals &lt;- synth_animals |&gt;\n  summarize(\n    ENDANGERED = sum(endangered),\n    CRITICAL = sum(critical),\n    VULNERABLE = sum(vulnerable),\n    THREATENED = sum(threat)\n  ) |&gt;\n  mutate(\n    Source = \"Synthetic\",\n    Country = \"Australia\",\n    Species = \"Mammals\"\n  )\nagg_animals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\nSource\nCountry\nSpecies\n\n\n\n\n37\n6\n65\n92\nSynthetic\nAustralia\nMammals\n\n\n\n\n\n\nAnd we can check how closely they match the macro-level count data:\n\n# First the synthetic data\nagg_animals_long &lt;- agg_animals |&gt; pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n)\n# Now the actual counts\nam_counts_long &lt;- wide_df |&gt;\n  filter(\n    (Species == \"Mammals\") & (Country == \"Australia\")\n  ) |&gt;\n  select(-TOT_KNOWN) |&gt;\n  mutate(Source = \"OECD\") |&gt;\n  pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n  )\n# And combine\nagg_df &lt;- bind_rows(agg_animals_long, am_counts_long)\nagg_df\n\n\n\n\n\nSource\nCountry\nSpecies\nvar\nvalue\n\n\n\n\nSynthetic\nAustralia\nMammals\nENDANGERED\n37\n\n\nSynthetic\nAustralia\nMammals\nCRITICAL\n6\n\n\nSynthetic\nAustralia\nMammals\nVULNERABLE\n65\n\n\nSynthetic\nAustralia\nMammals\nTHREATENED\n92\n\n\nOECD\nAustralia\nMammals\nENDANGERED\n41\n\n\nOECD\nAustralia\nMammals\nCRITICAL\n9\n\n\nOECD\nAustralia\nMammals\nVULNERABLE\n57\n\n\nOECD\nAustralia\nMammals\nTHREATENED\n107\n\n\n\n\n\n\nAnd visualize this closeness:\n\nggplot(agg_df, aes(x=var, y=value, fill=Source)) +\n  geom_bar(stat='identity', position='dodge') +\n  dsan_theme() +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)\n  )\n\n\n\n\n\n\n\n\nNot bad! If we repeat this procedure for all (country, species) pairs, we‚Äôll get an individual animal-level (micro-level) dataset! There are several efficient ways to do this, but for the sake of getting it done easily we can just use a loop. However, we‚Äôll have to be careful about missing values."
  },
  {
    "objectID": "posts/synthetic-data/index.html#footnotes",
    "href": "posts/synthetic-data/index.html#footnotes",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe thumbnail image for this post is from MIT Sloan‚Äôs primer on synthetic data.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/images-in-qmd/index.html",
    "href": "posts/images-in-qmd/index.html",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "href": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "href": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "title": "Including Images in .qmd Files",
    "section": "If You‚Äôre Unsure How To Save The Image File Locally",
    "text": "If You‚Äôre Unsure How To Save The Image File Locally\nSometimes, for example, students want to take a photo or scan of a written document, or a screenshot, and include that in a .qmd file. In that case, the difficult part is not knowing the above syntax but knowing how to get the image onto your drive in the first place.\nWhen I want to include screenshots, for example, I tend to use the Mac shortcut cmd+shift+F4, which lets me take a screenshot of some portion of the screen, and then I open the saved screenshot and choose ‚ÄúAdd to Photos‚Äù. Finally, I open up the Photos app and export the image to the images subdirectory mentioned above.\nWhen I want to include photos taken from my phone, there are a lot of different approaches based on different phone types and operating systems, but in general in my case I tend to export the photo to Google Photos or Dropbox, and then download the photo from photos.google.com or dropbox.com and include it in my .qmd file that way."
  },
  {
    "objectID": "posts/liwc/index.html",
    "href": "posts/liwc/index.html",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#the-text-files",
    "href": "posts/liwc/index.html#the-text-files",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "href": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Converting Into Python Regular Expressions",
    "text": "Converting Into Python Regular Expressions\nUsing them in this raw format is a bit tricky, however, since they are formatted not as individual words but as regular expressions, which will match entire families of positive and negative words. For example, 032-negemo.txt contains the entry troubl*, which will therefore match the words trouble, troubles, troubling, and so on.\nSo, to work with these files in Python, we‚Äôll need to load the .txt files but then convert each entry into a regular expression object. This can be done using the following collection of functions:\n\nimport re\ndef load_liwc_list(filepath):\n    \"\"\"\n    :return: A list of words loaded from the file at `fpath`\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        words = infile.read().split()\n    return words\n\ndef liwc_to_regex(liwc_list):\n    \"\"\"\n    Converts LIWC expression list into Python regular expression\n    \"\"\"\n    wildcard_reg = [w.replace('*', r'[^\\s]*') for w in liwc_list]\n    reg_str = r'\\b(' + '|'.join(wildcard_reg) + r')\\b'\n    return reg_str\n\ndef num_matches(reg_str, test_str):\n    num_matches = len(re.findall(reg_str,test_str))\n    return num_matches\n\ndef file_to_regex(filepath):\n    liwc_list = load_liwc_list(filepath)\n    liwc_regex_list = liwc_to_regex(liwc_list)\n    return liwc_regex_list\n\n# You can call the following helper function if\n# you'd like to see the full regular expression\ndef print_regex(regex_str, wrap_col=70):\n    import textwrap\n    print(textwrap.fill(regex_str, wrap_col))\n\nWe can use these functions to load the .txt files and create lists of regex (Regular Expression) objects from them:\n\npos_fpath = \"./assets/liwc/031-posemo.txt\"\npos_regex = file_to_regex(pos_fpath)\nneg_fpath = \"./assets/liwc/032-negemo.txt\"\nneg_regex = file_to_regex(neg_fpath)\n# Uncomment this line to see the full regular expression\n#print_regex(neg_regex)\nprint_regex(neg_regex[:140])\n\n\\b(dismay[^\\s]*|ignorant|poorest|tragic|disreput[^\\s]*|ignore|poorly|t\nrauma[^\\s]*|abandon[^\\s]*|diss|ignored|poorness[^\\s]*|trembl[^\\s]*|abu"
  },
  {
    "objectID": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "href": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Using the Regular Expressions to Generate Sentiment Scores",
    "text": "Using the Regular Expressions to Generate Sentiment Scores\nAnd now we can use these generated regular expressions to count the number of times ‚Äúpositive‚Äù and ‚Äúnegative‚Äù words appear in our string! Here we provide two final helper functions for accomplishing this:\n\ndef extract_sentiment_data(text):\n    # First compute positive sentiment using pos_reg\n    pos_count = num_matches(pos_regex, text)\n    # Then negative sentiment using neg_reg\n    neg_count = num_matches(neg_regex, text)\n    # And finally the overall sentiment score as the difference\n    sentiment = pos_count - neg_count\n    return {\n        'pos': pos_count,\n        'neg': neg_count,\n        'sentiment': sentiment\n    }\n\ndef compute_sentiment(text):\n    full_results = extract_sentiment_data(text)\n    # Return just the overall sentiment score\n    return full_results['sentiment']\n\nAnd here we test these helper functions out by creating positive, negative, and neutral test strings and checking the results for these strings:\n\nneg_test_str = \"Python is terrible, I hate Python, I despise Python\"\nneg_str_results = extract_sentiment_data(neg_test_str)\nprint(f\"{neg_test_str}\\n{neg_str_results}\")\npos_test_str = \"Python is wonderful, I love Python, I adore Python\"\npos_str_results = extract_sentiment_data(pos_test_str)\nprint(f\"{pos_test_str}\\n{pos_str_results}\")\nneutral_test_str = \"Python is ok, Python is mid, I guess I can do Python maybe\"\nneutral_str_results = extract_sentiment_data(neutral_test_str)\nprint(f\"{neutral_test_str}\\n{neutral_str_results}\")\n\nPython is terrible, I hate Python, I despise Python\n{'pos': 0, 'neg': 3, 'sentiment': -3}\nPython is wonderful, I love Python, I adore Python\n{'pos': 3, 'neg': 0, 'sentiment': 3}\nPython is ok, Python is mid, I guess I can do Python maybe\n{'pos': 1, 'neg': 0, 'sentiment': 1}"
  },
  {
    "objectID": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "href": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Computing Sentiment Scores for a DataFrame Column",
    "text": "Computing Sentiment Scores for a DataFrame Column\nEven though above we printed out the full results of each sentiment computation (by calling extract_sentiment_data(), which returns a dictionary containing the results), if you have a DataFrame with a text column that you‚Äôd like to perform sentiment analysis on, you can just use the simpler compute_sentiment() function to obtain a single number, like in the following code:\n\nimport pandas as pd\ntext_df = pd.DataFrame({\n    'text_id': [1,2,3],\n    'text': [neg_test_str, pos_test_str, neutral_test_str]\n})\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n\n\n\n\n\n\n\n\ntext_df['sentiment'] = text_df['text'].apply(compute_sentiment)\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\nsentiment\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n-3\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n3\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n1"
  },
  {
    "objectID": "posts/liwc/index.html#footnotes",
    "href": "posts/liwc/index.html#footnotes",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd, in case you want to use it in the future, you can download the entire set of word lists as a zip file here‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/datasets-for-viz/index.html",
    "href": "posts/datasets-for-viz/index.html",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "",
    "text": "There are lots of datasets out there, to the point that it can be scary trawling through e.g.¬†the (at time of writing) 1,780 datasets listed in the Data is Plural archive. This point of this post, therefore, is to provide a ‚Äúcurated‚Äù collection of datasets specifically chosen for how well they lend themselves to various methods of visualization. For the section of DSAN 5200: Advanced Data Visualization that I‚Äôm teaching this semester (Section 03), I‚Äôll be drawing on these datasets for demonstrations of how we can produce visual representations from raw numeric or text data."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#life-expectancy-by-country",
    "href": "posts/datasets-for-viz/index.html#life-expectancy-by-country",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Life Expectancy By Country",
    "text": "Life Expectancy By Country\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Left: The life expectancy gap between males and females in 2015, by country. Right: The change in life expectancy between 2000 and 2015, by country. Both from Nathan Yau‚Äôs One Dataset, Visualized 25 Ways.\n\n\nThe data, from the World Health Organization‚Äôs Global Health Observatory.\n\nType: Panel (Country x Year)\n\nThis is the dataset behind Nathan Yau‚Äôs blog post One Dataset, Visualized 25 Ways. This post provides a great example of how data doesn‚Äôt ‚Äúspeak for itself‚Äù, and how visualization choices‚Äîwhat aspects to highlight and what aspects to exclude‚Äîcan have a big impact on how underlying patterns in the data are extracted, communicated, and understood by an audience."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#perception-of-probability-words",
    "href": "posts/datasets-for-viz/index.html#perception-of-probability-words",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Perception of Probability Words",
    "text": "Perception of Probability Words\n\n\n\nFigure¬†2: Joyplot of the association between phrases and numeric values, from GitHub repo of reddit user zonination\n\n\n\nType: Cross-sectional (one row per respondent)\nSurvey of 123 redditors, based on a set of charts from an earlier (since-declassified) CIA publication on ‚ÄúWords of Estimative Probability‚Äù, data here\nVisualization of a precursor to this dataset on D3 Graph Gallery\nVisualization of this data, along with additional contextual details, from Illinois Computer Science professor Wade Fagen-Ulmschneider\nAnalysis from Visual Capitalist\n\nThis is one of my favorite datasets for visualization, and just for pondering in general, because it brings out the tension between the ‚Äúfuzziness‚Äù of human decision-making and the numeric precision which is required for the algorithmic decision-making carried out by computers. Thinking through the visualizations of the above data on this page ‚Äúin reverse‚Äù can help us understand how humans perceive the outputs of probabilistic machine learning algorithms: there is a sense in which an ‚Äúaverage‚Äù human (generalizing speculatively from this small-\\(N\\) survey!) will interpret an algorithm‚Äôs output of 0.95 as an ‚Äúalmost certain‚Äù result, while a result between 0.15 and 0.30 will be interpreted by ~50% of humans as a ‚Äúprobably not‚Äù result."
  },
  {
    "objectID": "posts/datasets-for-viz/index.html#banknote-faces",
    "href": "posts/datasets-for-viz/index.html#banknote-faces",
    "title": "Jeff‚Äôs Big Bucket of Datasets for Visualization",
    "section": "Banknote Faces",
    "text": "Banknote Faces\n\n\n\nFigure¬†3: From the pudding.cool article Who‚Äôs In Your Wallet?, visualizing the gaps between when a public figure died and when their face was put on a country‚Äôs banknotes.\n\n\n\nType: Cross-sectional (one row per person)\nData available via the GitHub repo for the article\n\npudding.cool is one of my favorite websites of all time, and has dozens and dozens of fascinating datasets and analyses, but this one I think is really straightforward yet lends itself to lots of different visualizations, like the one presented above plotting the gaps in time between when a person died and when their face was placed on a country‚Äôs banknotes. One of my favorite aspects about this plot is how it leads the viewer (and by the viewer I specifically mean me! But, I would guess that others may be intrigued by this aspect as well?) to want to know what the ‚Äúcorner cases‚Äù represent: who is the one person who died thousands of years before they were placed on a bill? Or, who are the 10 people who were still alive when their face was placed on a banknote? I guess you‚Äôll have to go in and explore the data (or read the pudding.cool article) to find out!"
  },
  {
    "objectID": "posts/mounting-gu-domains/index.html",
    "href": "posts/mounting-gu-domains/index.html",
    "title": "Mounting Your GU Domains Space as a Local Drive",
    "section": "",
    "text": "You can use a Mac App called Mountain Duck to mount your GU Domains space as a disk on your local computer, so that you can upload/download/view/modify files on your GU Domains space the same way you work with files on your local computer.\nOn my Mac, for example, I have my GU Domains server space mounted so that my Finder sidebar looks as follows:\n\n\n\n\n\nSo that when I open this Georgetown Domains ‚Äúdrive‚Äù, I see the following:"
  }
]