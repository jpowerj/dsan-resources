[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources for DSAN Students",
    "section": "",
    "text": "This page contains some general resources for DSAN students, which I hope can be useful across multiple different DSAN courses and/or in your future careers! ü§©\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Synthetic Microdata from Aggregated Count Data\n\n\n\n\n\n\n\nGeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nUsing LIWC for Document-Level Sentiment Analysis\n\n\n\n\n\n\n\nText Analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nEstablishing Global Settings for R in VSCode\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nKangheng Liu, Jeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nMounting Your GU Domains Space as a Local Drive\n\n\n\n\n\n\n\nGU Domains\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nIncluding Images in .qmd Files\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJeff Jacobs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rprofile-globals/index.html",
    "href": "posts/rprofile-globals/index.html",
    "title": "Establishing Global Settings for R in VSCode",
    "section": "",
    "text": "Awesome student Kangheng Liu figured out the following, which will be helpful for students who use the dark (default!) theme for VSCode but find that their R plots show up as black labels on a black background, so that they are not readable:\nThe IRkernel actually calls the R default startup process. The .Rprofile works whether inside or outside of jupyter.\nThe problem was R‚Äôs logic in sourcing the .Rprofile file. It won‚Äôt load libraries/packages in starting R kernel process. To pass background color as white, we need to write a hook for that. Search for hook in the link for explanations.\nThe coding is as below. in .Rprofile at home folder (or project folder, will take presendence if any):\nsetHook(packageEvent(\"grDevices\", \"onLoad\"),\n        function(...) grDevices::pdf.options(bg=\"white\"))\nThis will pass bg=\"white\" to grDevices, which is the R graphics device package.\nThank you Kangheng, on behalf of the DSAN students, for discovering this!"
  },
  {
    "objectID": "posts/liwc/index.html",
    "href": "posts/liwc/index.html",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#the-text-files",
    "href": "posts/liwc/index.html#the-text-files",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "",
    "text": "You can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose ‚ÄúSave Link As‚Ä¶‚Äù to download)1:\n\n031-posemo.txt\n032-negemo.txt"
  },
  {
    "objectID": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "href": "posts/liwc/index.html#converting-into-python-regular-expressions",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Converting Into Python Regular Expressions",
    "text": "Converting Into Python Regular Expressions\nUsing them in this raw format is a bit tricky, however, since they are formatted not as individual words but as regular expressions, which will match entire families of positive and negative words. For example, 032-negemo.txt contains the entry troubl*, which will therefore match the words trouble, troubles, troubling, and so on.\nSo, to work with these files in Python, we‚Äôll need to load the .txt files but then convert each entry into a regular expression object. This can be done using the following collection of functions:\n\nimport re\ndef load_liwc_list(filepath):\n    \"\"\"\n    :return: A list of words loaded from the file at `fpath`\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        words = infile.read().split()\n    return words\n\ndef liwc_to_regex(liwc_list):\n    \"\"\"\n    Converts LIWC expression list into Python regular expression\n    \"\"\"\n    wildcard_reg = [w.replace('*', r'[^\\s]*') for w in liwc_list]\n    reg_str = r'\\b(' + '|'.join(wildcard_reg) + r')\\b'\n    return reg_str\n\ndef num_matches(reg_str, test_str):\n    num_matches = len(re.findall(reg_str,test_str))\n    return num_matches\n\ndef file_to_regex(filepath):\n    liwc_list = load_liwc_list(filepath)\n    liwc_regex_list = liwc_to_regex(liwc_list)\n    return liwc_regex_list\n\n# You can call the following helper function if\n# you'd like to see the full regular expression\ndef print_regex(regex_str, wrap_col=70):\n    import textwrap\n    print(textwrap.fill(regex_str, wrap_col))\n\nWe can use these functions to load the .txt files and create lists of regex (Regular Expression) objects from them:\n\npos_fpath = \"./assets/liwc/031-posemo.txt\"\npos_regex = file_to_regex(pos_fpath)\nneg_fpath = \"./assets/liwc/032-negemo.txt\"\nneg_regex = file_to_regex(neg_fpath)\n# Uncomment this line to see the full regular expression\n#print_regex(neg_regex)\nprint_regex(neg_regex[:140])\n\n\\b(dismay[^\\s]*|ignorant|poorest|tragic|disreput[^\\s]*|ignore|poorly|t\nrauma[^\\s]*|abandon[^\\s]*|diss|ignored|poorness[^\\s]*|trembl[^\\s]*|abu"
  },
  {
    "objectID": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "href": "posts/liwc/index.html#using-the-regular-expressions-to-generate-sentiment-scores",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Using the Regular Expressions to Generate Sentiment Scores",
    "text": "Using the Regular Expressions to Generate Sentiment Scores\nAnd now we can use these generated regular expressions to count the number of times ‚Äúpositive‚Äù and ‚Äúnegative‚Äù words appear in our string! Here we provide two final helper functions for accomplishing this:\n\ndef extract_sentiment_data(text):\n    # First compute positive sentiment using pos_reg\n    pos_count = num_matches(pos_regex, text)\n    # Then negative sentiment using neg_reg\n    neg_count = num_matches(neg_regex, text)\n    # And finally the overall sentiment score as the difference\n    sentiment = pos_count - neg_count\n    return {\n        'pos': pos_count,\n        'neg': neg_count,\n        'sentiment': sentiment\n    }\n\ndef compute_sentiment(text):\n    full_results = extract_sentiment_data(text)\n    # Return just the overall sentiment score\n    return full_results['sentiment']\n\nAnd here we test these helper functions out by creating positive, negative, and neutral test strings and checking the results for these strings:\n\nneg_test_str = \"Python is terrible, I hate Python, I despise Python\"\nneg_str_results = extract_sentiment_data(neg_test_str)\nprint(f\"{neg_test_str}\\n{neg_str_results}\")\npos_test_str = \"Python is wonderful, I love Python, I adore Python\"\npos_str_results = extract_sentiment_data(pos_test_str)\nprint(f\"{pos_test_str}\\n{pos_str_results}\")\nneutral_test_str = \"Python is ok, Python is mid, I guess I can do Python maybe\"\nneutral_str_results = extract_sentiment_data(neutral_test_str)\nprint(f\"{neutral_test_str}\\n{neutral_str_results}\")\n\nPython is terrible, I hate Python, I despise Python\n{'pos': 0, 'neg': 3, 'sentiment': -3}\nPython is wonderful, I love Python, I adore Python\n{'pos': 3, 'neg': 0, 'sentiment': 3}\nPython is ok, Python is mid, I guess I can do Python maybe\n{'pos': 1, 'neg': 0, 'sentiment': 1}"
  },
  {
    "objectID": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "href": "posts/liwc/index.html#computing-sentiment-scores-for-a-dataframe-column",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Computing Sentiment Scores for a DataFrame Column",
    "text": "Computing Sentiment Scores for a DataFrame Column\nEven though above we printed out the full results of each sentiment computation (by calling extract_sentiment_data(), which returns a dictionary containing the results), if you have a DataFrame with a text column that you‚Äôd like to perform sentiment analysis on, you can just use the simpler compute_sentiment() function to obtain a single number, like in the following code:\n\nimport pandas as pd\ntext_df = pd.DataFrame({\n    'text_id': [1,2,3],\n    'text': [neg_test_str, pos_test_str, neutral_test_str]\n})\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n\n\n\n\n\n\n\n\ntext_df['sentiment'] = text_df['text'].apply(compute_sentiment)\ntext_df\n\n\n\n\n\n\n\n\ntext_id\ntext\nsentiment\n\n\n\n\n0\n1\nPython is terrible, I hate Python, I despise P...\n-3\n\n\n1\n2\nPython is wonderful, I love Python, I adore Py...\n3\n\n\n2\n3\nPython is ok, Python is mid, I guess I can do ...\n1"
  },
  {
    "objectID": "posts/liwc/index.html#footnotes",
    "href": "posts/liwc/index.html#footnotes",
    "title": "Using LIWC for Document-Level Sentiment Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd, in case you want to use it in the future, you can download the entire set of word lists as a zip file here‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/images-in-qmd/index.html",
    "href": "posts/images-in-qmd/index.html",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "href": "posts/images-in-qmd/index.html#if-you-already-have-the-image-file-saved-locally",
    "title": "Including Images in .qmd Files",
    "section": "",
    "text": "If you already have an image file ready to be included in your .qmd file, you can include it in a .qmd file using the following syntax:\n![Image caption](/path/to/image.jpg)\nSo, for example, if the image has the filename my_image.png, and it is saved in the same folder as the .qmd file you are trying to include it in, then the syntax would look like:\n![Image caption](my_image.png)\n\n\nIf you are trying to create a .qmd document which uses a lot of images, it is good practice to create an images subdirectory in the folder where your .qmd file exists, so that the main directory does not get cluttered by all of the image files.\nSo, if we moved our my_image.png file into the images subdirectory, our syntax for including the image would now look like:\n![Image caption](images/my_image.png)"
  },
  {
    "objectID": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "href": "posts/images-in-qmd/index.html#if-youre-unsure-how-to-save-the-image-file-locally",
    "title": "Including Images in .qmd Files",
    "section": "If You‚Äôre Unsure How To Save The Image File Locally",
    "text": "If You‚Äôre Unsure How To Save The Image File Locally\nSometimes, for example, students want to take a photo or scan of a written document, or a screenshot, and include that in a .qmd file. In that case, the difficult part is not knowing the above syntax but knowing how to get the image onto your drive in the first place.\nWhen I want to include screenshots, for example, I tend to use the Mac shortcut cmd+shift+F4, which lets me take a screenshot of some portion of the screen, and then I open the saved screenshot and choose ‚ÄúAdd to Photos‚Äù. Finally, I open up the Photos app and export the image to the images subdirectory mentioned above.\nWhen I want to include photos taken from my phone, there are a lot of different approaches based on different phone types and operating systems, but in general in my case I tend to export the photo to Google Photos or Dropbox, and then download the photo from photos.google.com or dropbox.com and include it in my .qmd file that way."
  },
  {
    "objectID": "posts/synthetic-data/index.html",
    "href": "posts/synthetic-data/index.html",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "href": "posts/synthetic-data/index.html#macro-level-vs.-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "",
    "text": "source(\"../../_globals.r\")\n\n\nBefore we look at how to ‚Äúdecompose‚Äù the macro-level OECD dataset we have into micro-level data on individual animal types, let‚Äôs motivate why we have to do all this work in the first place!\nImagine that you are the prime minister of a tiny country, which has only 10 citizens, who are split across two provinces (3 citizens live in Province A, 4 live in Province B, and the remaining 3 live in Province C). You‚Äôre hoping to study the healthiness of the people in your country. You would like to have a dataset containing 10 observations, one for each citizen in your country, but the data was collected anonymously, to protect each citizen‚Äôs privacy.\nThis means that, instead of having a dataset with 10 rows, one for each citizen, instead you have the following dataset which contains counts of various features across the three provinces:\n\nlibrary(tidyverse)\nsick_df &lt;- tribble(\n    ~Province, ~Total_Population, ~Fever, ~High_Fever, ~Cough,\n    \"A\", 3, 2, 1, 1,\n    \"B\", 4, 1, 1, 2,\n    \"C\", 3, 1, 1, 3,\n)\nsick_df\n\n\n\n\n\nProvince\nTotal_Population\nFever\nHigh_Fever\nCough\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nNow, think about whether (or how) you could potentially decompose this aggregated count data down into individual observations of each citizen‚Ä¶\nThe issue in trying to do this is: we don‚Äôt have any information on whether individual people in each province might have multiple conditions! For example, we know that High_Fever is a more severe version of Fever, so that all citizens who have a high fever also have a fever, but not all citizens who have a fever have a high fever.\nSeparately, a citizen might have both a cough and a fever, or just a cough, or just a fever, or none of these (they may be perfectly healthy).\nBecause we don‚Äôt have this info, sadly, there are many possible datasets which could have produced this aggregate data. For example, the following individual data could have produced it:\n\n\nCode\nmicro_df_1 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 1,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 0,\n    4, 'B', 1, 1, 1,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 0,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_1\n\n\n\n\nMicro-Level Dataset #1 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n1\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n0\n\n\n4\nB\n1\n1\n1\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see by grouping and summing:\n\nmicro_df_1 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #1, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\n(This is exactly the macro-level dataset was saw above)\nBut this totally different micro-level dataset could also have generated the exact same macro-level count dataset:\n\n\nCode\nmicro_df_2 &lt;- tribble(\n    ~citizen_id, ~Province, ~Fever, ~High_Fever, ~Cough,\n    1, 'A', 1, 1, 0,\n    2, 'A', 1, 0, 0,\n    3, 'A', 0, 0, 1,\n    4, 'B', 1, 1, 0,\n    5, 'B', 0, 0, 1,\n    6, 'B', 0, 0, 1,\n    7, 'B', 0, 0, 0,\n    8, 'C', 1, 1, 1,\n    9, 'C', 0, 0, 1,\n    10, 'C', 0, 0, 1\n)\nmicro_df_2\n\n\n\n\nMicro-Level Dataset #2 \n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n1\nA\n1\n1\n0\n\n\n2\nA\n1\n0\n0\n\n\n3\nA\n0\n0\n1\n\n\n4\nB\n1\n1\n0\n\n\n5\nB\n0\n0\n1\n\n\n6\nB\n0\n0\n1\n\n\n7\nB\n0\n0\n0\n\n\n8\nC\n1\n1\n1\n\n\n9\nC\n0\n0\n1\n\n\n10\nC\n0\n0\n1\n\n\n\n\n\n\nWhich we can see generates the same data, by applying the same aggregation we applied to Micro-Level Dataset #1:\n\nmicro_df_2 |&gt;\n  group_by(Province) |&gt;\n  summarize(\n    n(), sum(Fever), sum(High_Fever), sum(Cough)\n  )\n\n\n\nMicro-Level Dataset #2, Aggregated by Province \n\n\nProvince\nn()\nsum(Fever)\nsum(High_Fever)\nsum(Cough)\n\n\n\n\nA\n3\n2\n1\n1\n\n\nB\n4\n1\n1\n2\n\n\nC\n3\n1\n1\n3\n\n\n\n\n\n\nHopefully that helps to indicate the issue! Without knowing the specific joint or conditional probabilities, we don‚Äôt have enough information to fully know the underlying micro-level data.\nAlso, notice how the two micro-datasets are not equivalent in the context of knowing how healthy the population is üò± in Micro-Level Dataset #2, 9 out of 10 citizens have an illness (only one person, the citizen with id 7, is fully healthy), whereas in Micro-Level Dataset #1 only 7 out of 10 citizens have an illness (the citizens with ids 3, 6, and 7 are fully healthy). We can see this using a bit of code, to extract just the fully-healthy citizens from each dataset:\n\nhealthy_1 &lt;- micro_df_1 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_1\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n3\nA\n0\n0\n0\n\n\n6\nB\n0\n0\n0\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\nhealthy_2 &lt;- micro_df_2 |&gt; filter(\n    (Fever == 0) & (High_Fever == 0) & (Cough == 0)\n)\nhealthy_2\n\n\n\n\n\ncitizen_id\nProvince\nFever\nHigh_Fever\nCough\n\n\n\n\n7\nB\n0\n0\n0\n\n\n\n\n\n\nHowever, if we run into this problem in the real world, all hope is not lost! We know that some micro-level datasets are impossible: for example, we know that a dataset where everyone has all the illnesses would not aggregate to our macro-level data. So, we could construct a synthetic dataset as something like an average over all of the possible micro-level datasets that could be ‚Äúunderneath‚Äù the macro-level dataset we actually observed. I‚Äôll call this the statistically-principled-and-possible-but-brutally-difficult approach, though, because this gets us into the territory of what is called Ecological Inference, and statistical models for doing this get very complicated very quickly (requiring big scary-looking textbooks).\nSo instead, in the rest of the writeup, we‚Äôll just look at how to construct one such ‚Äúpossible‚Äù synthetic dataset, in a way that is smarter than the greedy approach we used to construct Micro-Level Dataset #1 above, but not as smart as the full-on statistically-principled approach."
  },
  {
    "objectID": "posts/synthetic-data/index.html#macro-level-count-data",
    "href": "posts/synthetic-data/index.html#macro-level-count-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Macro-Level Count Data",
    "text": "Macro-Level Count Data\nSo, say you have a dataset that looks like the following, for example, obtained from the OECD‚Äôs Data Explorer portal1:\n\nlibrary(tidyverse)\nmacro_df &lt;- read_csv(\"assets/wild_life_cleaned.csv\")\nmacro_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIUCN\nIUCN Category\nSPEC\nSpecies\nCOU\nCountry\nValue\n\n\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUS\nAustralia\n377\n\n\nENDANGERED\nNumber of endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n41\n\n\nCRITICAL\nNumber of critically endangered species\nMAMMAL\nMammals\nAUS\nAustralia\n9\n\n\nVULNERABLE\nNumber of vulnerable species\nMAMMAL\nMammals\nAUS\nAustralia\n57\n\n\nTHREATENED\nTotal number of threatened species\nMAMMAL\nMammals\nAUS\nAustralia\n107\n\n\nTOT_KNOWN\nTotal number of known species\nMAMMAL\nMammals\nAUT\nAustria\n104\n\n\n\n\n\n\nOnce we transform it from long to wide format, we‚Äôll be able to see how this dataset provides data on (country-species) pairs (e.g., Mammals in Australia), not on individual types of animals (e.g., Kangaroos in Australia):\n\nwide_df &lt;- macro_df |&gt;\n  select(-c(`IUCN Category`, COU, SPEC)) |&gt;\n  pivot_wider(\n    names_from = IUCN,\n    values_from = Value\n  ) |&gt;\n  select(-c(THREAT_PERCENT))\nwide_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nCountry\nTOT_KNOWN\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\n\n\n\n\nMammals\nAustralia\n377\n41\n9\n57\n107\n\n\nMammals\nAustria\n104\n6\n4\n17\n27\n\n\nMammals\nBelgium\n84\n2\n4\n12\n18\n\n\nMammals\nCanada\n222\n11\n11\n33\n55\n\n\nMammals\nCzech Republic\n92\n1\n10\n4\n15\n\n\nMammals\nDenmark\n71\n2\n1\n6\n9\n\n\n\n\n\n\nAlthough some methods like Decision Trees can handle both classification and regression tasks, there are other methods like ARM where using this type of count data can be difficult, since most of the methods and libraries only support 0/1 binary matrices.\nBut, if you think about it for long enough, you‚Äôll realize that we could use this aggregated count data to generate simulated versions of the underlying microdata.\nFor example: notice how this dataset tells us that there are 377 known mammals in Australia, and that of these 377,\n\n41 are Endangered\n9 are Critical,\n57 are Vulnerable, and\n107 are Threatened.\n\nIt relates back to exercises we did early in DSAN 5100, actually: our dataset here provides us with marginal frequencies, but does not give us joint frequencies or conditional frequencies. For example, we don‚Äôt know how many mammals in Australia are both threatened and critical, even though it seems like the number of threatened species is greater than the number of critical species, at least in the rows we can see above.\nIf we did have all of this data, then we could just mathematically derive the number of animals in each possible combination of categories: for example, the exact number of mammals in Canada which are threatened and vulnerable but not endangered or critical. However, since we don‚Äôt know the number of animals in all possible categories (combinations of the four features), we‚Äôre in the same situation as in the simplified example above. So, we‚Äôll have to construct a synthetic dataset, in this case a micro-level dataset of individual animal types which matches the count data in the existing macro-level dataset as best as possible."
  },
  {
    "objectID": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "href": "posts/synthetic-data/index.html#approximating-the-micro-level-data",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Approximating the Micro-Level Data",
    "text": "Approximating the Micro-Level Data\nThe key insight is just recognizing that, since we have counts as well as totals in each row, we can construct probabilities of a given animal type having a given property (using the na√Øve definition of probability, and assuming that each property is independent, which is a terrible assumption in this case, but getting rid of this assumption brings us back into scary math world).\nFor example: looking at the four bullet points above, relative to the fact that there are 377 total types of mammals in Australia, we can generate a set of probabilities that a given (arbitrarily-chosen) animal type in Australia has each of these properties. Letting \\(S\\) be a Random Variable representing an animal‚Äôs Species and \\(C\\) a Random Variable representing an animal‚Äôs Country, and then letting \\(\\texttt{M}\\) stand for ‚ÄúMammals‚Äù and \\(\\texttt{A}\\) stand for ‚ÄúAustralia‚Äù:\n\\[\n\\begin{align*}\n\\Pr(\\text{Endangered} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{41}{377} &\\approx 0.109 \\\\\n\\Pr(\\text{Critical} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{9}{377} &\\approx 0.024 \\\\\n\\Pr(\\text{Vulnerable} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{57}{377} &\\approx 0.151 \\\\\n\\Pr(\\text{Threatened} \\mid S = \\texttt{M}, C = \\texttt{A}) = \\frac{107}{377} &\\approx 0.284\n\\end{align*}\n\\]\nWhich means that we could now generate a dataset of individual animals which would approximate (under our very questionable assumptions of independence, at least) the aggregate information we have. For this specific example of mammals in Australia, then, we could write code to generate data for mammals in Australia like the following:\n\nlibrary(Rlab)\nN &lt;- 377\np_endangered &lt;- 0.109\np_critical &lt;- 0.024\np_vulnerable &lt;- 0.151\np_threat &lt;- 0.284\nsynth_animals &lt;- tibble(\n    animal_id = seq(1, N),\n    endangered = rbern(N, p_endangered),\n    critical = rbern(N, p_critical),\n    vulnerable = rbern(N, p_vulnerable),\n    threat = rbern(N, p_threat)\n)\nsynth_animals |&gt; head()\n\n\n\n\n\nanimal_id\nendangered\ncritical\nvulnerable\nthreat\n\n\n\n\n1\n0\n0\n0\n1\n\n\n2\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n1\n\n\n5\n1\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n\n\n\n\n\n\nAnd now we can aggregate these individual ‚Äúcoin flips‚Äù to obtain counts:\n\nagg_animals &lt;- synth_animals |&gt;\n  summarize(\n    ENDANGERED = sum(endangered),\n    CRITICAL = sum(critical),\n    VULNERABLE = sum(vulnerable),\n    THREATENED = sum(threat)\n  ) |&gt;\n  mutate(\n    Source = \"Synthetic\",\n    Country = \"Australia\",\n    Species = \"Mammals\"\n  )\nagg_animals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENDANGERED\nCRITICAL\nVULNERABLE\nTHREATENED\nSource\nCountry\nSpecies\n\n\n\n\n37\n6\n65\n92\nSynthetic\nAustralia\nMammals\n\n\n\n\n\n\nAnd we can check how closely they match the macro-level count data:\n\n# First the synthetic data\nagg_animals_long &lt;- agg_animals |&gt; pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n)\n# Now the actual counts\nam_counts_long &lt;- wide_df |&gt;\n  filter(\n    (Species == \"Mammals\") & (Country == \"Australia\")\n  ) |&gt;\n  select(-TOT_KNOWN) |&gt;\n  mutate(Source = \"OECD\") |&gt;\n  pivot_longer(\n    -c(Source, Species, Country),\n    names_to = \"var\",\n    values_to = \"value\"\n  )\n# And combine\nagg_df &lt;- bind_rows(agg_animals_long, am_counts_long)\nagg_df\n\n\n\n\n\nSource\nCountry\nSpecies\nvar\nvalue\n\n\n\n\nSynthetic\nAustralia\nMammals\nENDANGERED\n37\n\n\nSynthetic\nAustralia\nMammals\nCRITICAL\n6\n\n\nSynthetic\nAustralia\nMammals\nVULNERABLE\n65\n\n\nSynthetic\nAustralia\nMammals\nTHREATENED\n92\n\n\nOECD\nAustralia\nMammals\nENDANGERED\n41\n\n\nOECD\nAustralia\nMammals\nCRITICAL\n9\n\n\nOECD\nAustralia\nMammals\nVULNERABLE\n57\n\n\nOECD\nAustralia\nMammals\nTHREATENED\n107\n\n\n\n\n\n\nAnd visualize this closeness:\n\nggplot(agg_df, aes(x=var, y=value, fill=Source)) +\n  geom_bar(stat='identity', position='dodge') +\n  dsan_theme() +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)\n  )\n\n\n\n\n\n\n\n\nNot bad! If we repeat this procedure for all (country, species) pairs, we‚Äôll get an individual animal-level (micro-level) dataset! There are several efficient ways to do this, but for the sake of getting it done easily we can just use a loop. However, we‚Äôll have to be careful about missing values."
  },
  {
    "objectID": "posts/synthetic-data/index.html#footnotes",
    "href": "posts/synthetic-data/index.html#footnotes",
    "title": "Generating Synthetic Microdata from Aggregated Count Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe thumbnail image for this post is from MIT Sloan‚Äôs primer on synthetic data.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/mounting-gu-domains/index.html",
    "href": "posts/mounting-gu-domains/index.html",
    "title": "Mounting Your GU Domains Space as a Local Drive",
    "section": "",
    "text": "You can use a Mac App called Mountain Duck to mount your GU Domains space as a disk on your local computer, so that you can upload/download/view/modify files on your GU Domains space the same way you work with files on your local computer.\nOn my Mac, for example, I have my GU Domains server space mounted so that my Finder sidebar looks as follows:\n\n\n\n\n\nSo that when I open this Georgetown Domains ‚Äúdrive‚Äù, I see the following:"
  }
]