{"title":"Using LIWC for Document-Level Sentiment Analysis","markdown":{"yaml":{"title":"Using LIWC for Document-Level Sentiment Analysis","image":"images/liwc.jpg","author":"Jeff Jacobs","institute":"<a href='mailto:jj1088@georgetown.edu' target='_blank'>`jj1088@georgetown.edu`</a>","date":"last-modified","format":{"html":{"df-print":"kable"}},"categories":["Text Analysis"]},"headingText":"The Text Files","containsRefs":false,"markdown":"\n\n\nYou can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose \"Save Link As...\" to download)^[And, in case you want to use it in the future, you can download the entire set of word lists as a zip file <a href='assets/liwc.zip' target='_blank' class='pe-1'>here</a>]:\n\n* <a href='assets/liwc/031-posemo.txt' target='_blank'>`031-posemo.txt`</a>\n* <a href='assets/liwc/032-negemo.txt' target='_blank'>`032-negemo.txt`</a>\n\n## Converting Into Python Regular Expressions\n\nUsing them in this raw format is a bit tricky, however, since they are formatted not as individual words but as *regular expressions*, which will match entire *families* of positive and negative words. For example, `032-negemo.txt` contains the entry `troubl*`, which will therefore match the words `trouble`, `troubles`, `troubling`, and so on.\n\nSo, to work with these files in Python, we'll need to load the .txt files but then convert each entry into a **regular expression object**. This can be done using the following collection of functions:\n\n```{python}\n#| label: sentiment-on-strings\nimport re\ndef load_liwc_list(filepath):\n    \"\"\"\n    :return: A list of words loaded from the file at `fpath`\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        words = infile.read().split()\n    return words\n\ndef liwc_to_regex(liwc_list):\n    \"\"\"\n    Converts LIWC expression list into Python regular expression\n    \"\"\"\n    wildcard_reg = [w.replace('*', r'[^\\s]*') for w in liwc_list]\n    reg_str = r'\\b(' + '|'.join(wildcard_reg) + r')\\b'\n    return reg_str\n\ndef num_matches(reg_str, test_str):\n    num_matches = len(re.findall(reg_str,test_str))\n    return num_matches\n\ndef file_to_regex(filepath):\n    liwc_list = load_liwc_list(filepath)\n    liwc_regex_list = liwc_to_regex(liwc_list)\n    return liwc_regex_list\n\n# You can call the following helper function if\n# you'd like to see the full regular expression\ndef print_regex(regex_str, wrap_col=70):\n    import textwrap\n    print(textwrap.fill(regex_str, wrap_col))\n```\n\nWe can use these functions to load the .txt files and create lists of regex (Regular Expression) objects from them:\n\n```{python}\n#| label: disp-regex\npos_fpath = \"./assets/liwc/031-posemo.txt\"\npos_regex = file_to_regex(pos_fpath)\nneg_fpath = \"./assets/liwc/032-negemo.txt\"\nneg_regex = file_to_regex(neg_fpath)\n# Uncomment this line to see the full regular expression\n#print_regex(neg_regex)\nprint_regex(neg_regex[:140])\n```\n\n## Using the Regular Expressions to Generate Sentiment Scores\n\nAnd now we can use these generated regular expressions to count the number of times \"positive\" and \"negative\" words appear in our string! Here we provide two final helper functions for accomplishing this:\n\n```{python}\n#| label: count-pos-neg\ndef extract_sentiment_data(text):\n    # First compute positive sentiment using pos_reg\n    pos_count = num_matches(pos_regex, text)\n    # Then negative sentiment using neg_reg\n    neg_count = num_matches(neg_regex, text)\n    # And finally the overall sentiment score as the difference\n    sentiment = pos_count - neg_count\n    return {\n        'pos': pos_count,\n        'neg': neg_count,\n        'sentiment': sentiment\n    }\n\ndef compute_sentiment(text):\n    full_results = extract_sentiment_data(text)\n    # Return just the overall sentiment score\n    return full_results['sentiment']\n```\n\nAnd here we test these helper functions out by creating positive, negative, and neutral *test strings* and checking the results for these strings:\n\n```{python}\n#| label: test-strings\nneg_test_str = \"Python is terrible, I hate Python, I despise Python\"\nneg_str_results = extract_sentiment_data(neg_test_str)\nprint(f\"{neg_test_str}\\n{neg_str_results}\")\npos_test_str = \"Python is wonderful, I love Python, I adore Python\"\npos_str_results = extract_sentiment_data(pos_test_str)\nprint(f\"{pos_test_str}\\n{pos_str_results}\")\nneutral_test_str = \"Python is ok, Python is mid, I guess I can do Python maybe\"\nneutral_str_results = extract_sentiment_data(neutral_test_str)\nprint(f\"{neutral_test_str}\\n{neutral_str_results}\")\n```\n\n## Computing Sentiment Scores for a DataFrame Column\n\nEven though above we printed out the full results of each sentiment computation (by calling `extract_sentiment_data()`, which returns a dictionary containing the results), if you have a **DataFrame** with a text column that you'd like to perform sentiment analysis on, you can just use the simpler `compute_sentiment()` function to obtain a single number, like in the following code:\n\n```{python}\n#| label: create-df\nimport pandas as pd\ntext_df = pd.DataFrame({\n    'text_id': [1,2,3],\n    'text': [neg_test_str, pos_test_str, neutral_test_str]\n})\ntext_df\n```\n\n```{python}\n#| label: sentiment-on-df\ntext_df['sentiment'] = text_df['text'].apply(compute_sentiment)\ntext_df\n```\n","srcMarkdownNoYaml":"\n\n## The Text Files\n\nYou can download the .txt files for positive and negative sentiment at the following links (click them to view the contents, or right click and choose \"Save Link As...\" to download)^[And, in case you want to use it in the future, you can download the entire set of word lists as a zip file <a href='assets/liwc.zip' target='_blank' class='pe-1'>here</a>]:\n\n* <a href='assets/liwc/031-posemo.txt' target='_blank'>`031-posemo.txt`</a>\n* <a href='assets/liwc/032-negemo.txt' target='_blank'>`032-negemo.txt`</a>\n\n## Converting Into Python Regular Expressions\n\nUsing them in this raw format is a bit tricky, however, since they are formatted not as individual words but as *regular expressions*, which will match entire *families* of positive and negative words. For example, `032-negemo.txt` contains the entry `troubl*`, which will therefore match the words `trouble`, `troubles`, `troubling`, and so on.\n\nSo, to work with these files in Python, we'll need to load the .txt files but then convert each entry into a **regular expression object**. This can be done using the following collection of functions:\n\n```{python}\n#| label: sentiment-on-strings\nimport re\ndef load_liwc_list(filepath):\n    \"\"\"\n    :return: A list of words loaded from the file at `fpath`\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        words = infile.read().split()\n    return words\n\ndef liwc_to_regex(liwc_list):\n    \"\"\"\n    Converts LIWC expression list into Python regular expression\n    \"\"\"\n    wildcard_reg = [w.replace('*', r'[^\\s]*') for w in liwc_list]\n    reg_str = r'\\b(' + '|'.join(wildcard_reg) + r')\\b'\n    return reg_str\n\ndef num_matches(reg_str, test_str):\n    num_matches = len(re.findall(reg_str,test_str))\n    return num_matches\n\ndef file_to_regex(filepath):\n    liwc_list = load_liwc_list(filepath)\n    liwc_regex_list = liwc_to_regex(liwc_list)\n    return liwc_regex_list\n\n# You can call the following helper function if\n# you'd like to see the full regular expression\ndef print_regex(regex_str, wrap_col=70):\n    import textwrap\n    print(textwrap.fill(regex_str, wrap_col))\n```\n\nWe can use these functions to load the .txt files and create lists of regex (Regular Expression) objects from them:\n\n```{python}\n#| label: disp-regex\npos_fpath = \"./assets/liwc/031-posemo.txt\"\npos_regex = file_to_regex(pos_fpath)\nneg_fpath = \"./assets/liwc/032-negemo.txt\"\nneg_regex = file_to_regex(neg_fpath)\n# Uncomment this line to see the full regular expression\n#print_regex(neg_regex)\nprint_regex(neg_regex[:140])\n```\n\n## Using the Regular Expressions to Generate Sentiment Scores\n\nAnd now we can use these generated regular expressions to count the number of times \"positive\" and \"negative\" words appear in our string! Here we provide two final helper functions for accomplishing this:\n\n```{python}\n#| label: count-pos-neg\ndef extract_sentiment_data(text):\n    # First compute positive sentiment using pos_reg\n    pos_count = num_matches(pos_regex, text)\n    # Then negative sentiment using neg_reg\n    neg_count = num_matches(neg_regex, text)\n    # And finally the overall sentiment score as the difference\n    sentiment = pos_count - neg_count\n    return {\n        'pos': pos_count,\n        'neg': neg_count,\n        'sentiment': sentiment\n    }\n\ndef compute_sentiment(text):\n    full_results = extract_sentiment_data(text)\n    # Return just the overall sentiment score\n    return full_results['sentiment']\n```\n\nAnd here we test these helper functions out by creating positive, negative, and neutral *test strings* and checking the results for these strings:\n\n```{python}\n#| label: test-strings\nneg_test_str = \"Python is terrible, I hate Python, I despise Python\"\nneg_str_results = extract_sentiment_data(neg_test_str)\nprint(f\"{neg_test_str}\\n{neg_str_results}\")\npos_test_str = \"Python is wonderful, I love Python, I adore Python\"\npos_str_results = extract_sentiment_data(pos_test_str)\nprint(f\"{pos_test_str}\\n{pos_str_results}\")\nneutral_test_str = \"Python is ok, Python is mid, I guess I can do Python maybe\"\nneutral_str_results = extract_sentiment_data(neutral_test_str)\nprint(f\"{neutral_test_str}\\n{neutral_str_results}\")\n```\n\n## Computing Sentiment Scores for a DataFrame Column\n\nEven though above we printed out the full results of each sentiment computation (by calling `extract_sentiment_data()`, which returns a dictionary containing the results), if you have a **DataFrame** with a text column that you'd like to perform sentiment analysis on, you can just use the simpler `compute_sentiment()` function to obtain a single number, like in the following code:\n\n```{python}\n#| label: create-df\nimport pandas as pd\ntext_df = pd.DataFrame({\n    'text_id': [1,2,3],\n    'text': [neg_test_str, pos_test_str, neutral_test_str]\n})\ntext_df\n```\n\n```{python}\n#| label: sentiment-on-df\ntext_df['sentiment'] = text_df['text'].apply(compute_sentiment)\ntext_df\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"default","title":"Using LIWC for Document-Level Sentiment Analysis","image":"images/liwc.jpg","author":"Jeff Jacobs","institute":"<a href='mailto:jj1088@georgetown.edu' target='_blank'>`jj1088@georgetown.edu`</a>","date":"last-modified","categories":["Text Analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}